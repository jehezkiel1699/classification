{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KLASIFIKASI MULTILABEL DENGAN SOFTMAX FIXX.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a9ed6fc801e74c79aaf3ce4100ccbb25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_07780876eff6416f9f88d946621bae09",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_72830165517143709920ed4567b28f3d",
              "IPY_MODEL_bf92422346a2474aabf5c87c95c9a745"
            ]
          }
        },
        "07780876eff6416f9f88d946621bae09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72830165517143709920ed4567b28f3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_adb81e0394fa4211bc842b697bddf399",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 229167,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 229167,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b95e0e04f7cf4a2f8fa75fe173112f21"
          }
        },
        "bf92422346a2474aabf5c87c95c9a745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_39e762d034a242ecad2ddbe70efc0551",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 229k/229k [00:01&lt;00:00, 143kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e2be6ce3017d45bb87390643add72650"
          }
        },
        "adb81e0394fa4211bc842b697bddf399": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b95e0e04f7cf4a2f8fa75fe173112f21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "39e762d034a242ecad2ddbe70efc0551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e2be6ce3017d45bb87390643add72650": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "caffe7d43be7435b96092a65ee9de896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7eeecfac3bea41b4afc82fff09a8fae1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_23d10a08359342b181ee945f9895b4c8",
              "IPY_MODEL_0ff39609684e43de9df3d27d8643c875"
            ]
          }
        },
        "7eeecfac3bea41b4afc82fff09a8fae1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "23d10a08359342b181ee945f9895b4c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_607df6ed11c54ff7b13691baf1065488",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 112,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 112,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f2898c6462904abd9c09e6d96b8fcac5"
          }
        },
        "0ff39609684e43de9df3d27d8643c875": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2a09522470564011867562456b2d67a3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 112/112 [00:00&lt;00:00, 155B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7ee60de3b07a40c99d071385daaa1d2f"
          }
        },
        "607df6ed11c54ff7b13691baf1065488": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f2898c6462904abd9c09e6d96b8fcac5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a09522470564011867562456b2d67a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7ee60de3b07a40c99d071385daaa1d2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "157d775d51114f2791b1f3f523b8f82d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_981a65c2e31f4086b9486305d8d49415",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d9b6d088049c43efb4d3608f1f9a74e4",
              "IPY_MODEL_f97e92889bc94679a25e99c64099b86e"
            ]
          }
        },
        "981a65c2e31f4086b9486305d8d49415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d9b6d088049c43efb4d3608f1f9a74e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6a3c3ccfa5204ab5b085c23602821aa5",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aed44bdbdeef4a50a586608fc17e979f"
          }
        },
        "f97e92889bc94679a25e99c64099b86e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4892145a924344639cfe4411706eb3b6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.00/2.00 [00:00&lt;00:00, 7.56B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fb3b19288948496b97e5cbc97e8f37ea"
          }
        },
        "6a3c3ccfa5204ab5b085c23602821aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aed44bdbdeef4a50a586608fc17e979f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4892145a924344639cfe4411706eb3b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fb3b19288948496b97e5cbc97e8f37ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b29e121107ba44959739976c3732944f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b31351461edc4bf4ab9cea14e5d8535d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_37c89ca5a714428798bb30a5b71298fb",
              "IPY_MODEL_83015367c68f4e6bae0c4f684c5a1cab"
            ]
          }
        },
        "b31351461edc4bf4ab9cea14e5d8535d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "37c89ca5a714428798bb30a5b71298fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bf08c0360cc34f38bff8991d6b89c1aa",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1534,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1534,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c8cac741b06f4e50850fb7181f75b503"
          }
        },
        "83015367c68f4e6bae0c4f684c5a1cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_33e98d53ac69409f924aed38f2ae97fa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.53k/1.53k [00:00&lt;00:00, 5.96kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f1258285c47944af8deb840cfd829103"
          }
        },
        "bf08c0360cc34f38bff8991d6b89c1aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c8cac741b06f4e50850fb7181f75b503": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33e98d53ac69409f924aed38f2ae97fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f1258285c47944af8deb840cfd829103": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1e470371265d41338de645c3631f39c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c1f0ccb29af84914b3a87ffd67f597a9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_70b7a8b01c8f4f2e8deec57c1c6786c9",
              "IPY_MODEL_acfd2a6786564e90a533b3da0c0bbe4d"
            ]
          }
        },
        "c1f0ccb29af84914b3a87ffd67f597a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "70b7a8b01c8f4f2e8deec57c1c6786c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_56ddf621152044b5a1ba0522c2c52922",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 497810400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 497810400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_793b56fdc72f427ca3510bee291585bf"
          }
        },
        "acfd2a6786564e90a533b3da0c0bbe4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6a686af7a39a4ebdbfc6ca5add7c5dff",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 498M/498M [00:09&lt;00:00, 53.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_22c3c58e2d664d62b5ce00cbb71dafa8"
          }
        },
        "56ddf621152044b5a1ba0522c2c52922": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "793b56fdc72f427ca3510bee291585bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6a686af7a39a4ebdbfc6ca5add7c5dff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "22c3c58e2d664d62b5ce00cbb71dafa8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOh4qGQg4RGx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf0d3f7a-3625-41d5-ff57-e2990d87cc53"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 8.0MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 53.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 51.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmjZzjjO4pG_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6db8e0c8-70e0-4c21-f5d3-f6060d3980c1"
      },
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "%matplotlib inline\n",
        "#%config InlineBackend.figure_format='retina'\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#print(device)\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "\n",
        "  print('there are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "  print('we will use the GPU: ', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "  print(\"No GPU available, using the CPU instead\")\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there are 1 GPU(s) available.\n",
            "we will use the GPU:  Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNC9dqpdiWlf",
        "outputId": "8f1339fb-399a-4a14-c4f2-fa07efde6a14"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeMA0T3uijEj"
      },
      "source": [
        "#df = pd.read_csv('/content/drive/MyDrive/Skripsi/datasetBeritaFIX.csv')\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/Skripsi/train_berita.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/Skripsi/test_berita.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV_2VUNCWQ2h"
      },
      "source": [
        "class_names = ['edukasi', 'tekno', 'sports', 'health', 'lifestyle']\n",
        "test_df['list_kategori'] = list(test_df[class_names].values)\n",
        "train_df['list_kategori'] = list(train_df[class_names].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB1bQIaCMjT1"
      },
      "source": [
        "train_df=train_df.drop(columns=class_names)\n",
        "test_df=test_df.drop(columns=class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGIMjOKlNly4"
      },
      "source": [
        "test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06JCZFtM5Yla",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164,
          "referenced_widgets": [
            "a9ed6fc801e74c79aaf3ce4100ccbb25",
            "07780876eff6416f9f88d946621bae09",
            "72830165517143709920ed4567b28f3d",
            "bf92422346a2474aabf5c87c95c9a745",
            "adb81e0394fa4211bc842b697bddf399",
            "b95e0e04f7cf4a2f8fa75fe173112f21",
            "39e762d034a242ecad2ddbe70efc0551",
            "e2be6ce3017d45bb87390643add72650",
            "caffe7d43be7435b96092a65ee9de896",
            "7eeecfac3bea41b4afc82fff09a8fae1",
            "23d10a08359342b181ee945f9895b4c8",
            "0ff39609684e43de9df3d27d8643c875",
            "607df6ed11c54ff7b13691baf1065488",
            "f2898c6462904abd9c09e6d96b8fcac5",
            "2a09522470564011867562456b2d67a3",
            "7ee60de3b07a40c99d071385daaa1d2f",
            "157d775d51114f2791b1f3f523b8f82d",
            "981a65c2e31f4086b9486305d8d49415",
            "d9b6d088049c43efb4d3608f1f9a74e4",
            "f97e92889bc94679a25e99c64099b86e",
            "6a3c3ccfa5204ab5b085c23602821aa5",
            "aed44bdbdeef4a50a586608fc17e979f",
            "4892145a924344639cfe4411706eb3b6",
            "fb3b19288948496b97e5cbc97e8f37ea"
          ]
        },
        "outputId": "b5a133a8-aae4-4730-9dae-d9a336957fba"
      },
      "source": [
        "PRE_TRAINED_MODEL_BAHASA =  'indobenchmark/indobert-base-p1'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_BAHASA)\n",
        "\n",
        "MAX_LEN = 400"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9ed6fc801e74c79aaf3ce4100ccbb25",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=229167.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "caffe7d43be7435b96092a65ee9de896",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "157d775d51114f2791b1f3f523b8f82d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2.0, style=ProgressStyle(description_wi…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMKmzUloup_Q"
      },
      "source": [
        "#Classify\n",
        "class DatasetBerita(Dataset):\n",
        "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      padding='max_length',\n",
        "      return_attention_mask=True,\n",
        "      truncation=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7X1Zo6UXY5J"
      },
      "source": [
        "val_df, test_df = train_test_split(\n",
        "  test_df,\n",
        "  test_size=0.5, # tes 50% val 50%\n",
        "  random_state=RANDOM_SEED\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKvREHGS5tb5"
      },
      "source": [
        "val_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ajG190o54ZK"
      },
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = DatasetBerita(\n",
        "    reviews=df.preprocessing_text.to_numpy(), #berdasarkan table di df\n",
        "    targets=df.list_kategori.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q48-zO0r55p0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fce2fdc1-ff3c-41b9-c5d5-6553f086e15c"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(val_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nrzJTXm7eX-"
      },
      "source": [
        "class Klasifikasi(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(Klasifikasi, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_BAHASA)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      return_dict=False\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9weqw687nTm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "b29e121107ba44959739976c3732944f",
            "b31351461edc4bf4ab9cea14e5d8535d",
            "37c89ca5a714428798bb30a5b71298fb",
            "83015367c68f4e6bae0c4f684c5a1cab",
            "bf08c0360cc34f38bff8991d6b89c1aa",
            "c8cac741b06f4e50850fb7181f75b503",
            "33e98d53ac69409f924aed38f2ae97fa",
            "f1258285c47944af8deb840cfd829103",
            "1e470371265d41338de645c3631f39c4",
            "c1f0ccb29af84914b3a87ffd67f597a9",
            "70b7a8b01c8f4f2e8deec57c1c6786c9",
            "acfd2a6786564e90a533b3da0c0bbe4d",
            "56ddf621152044b5a1ba0522c2c52922",
            "793b56fdc72f427ca3510bee291585bf",
            "6a686af7a39a4ebdbfc6ca5add7c5dff",
            "22c3c58e2d664d62b5ce00cbb71dafa8"
          ]
        },
        "outputId": "39b05255-3cb3-4fe4-eb1d-bdecad76f1be"
      },
      "source": [
        "model = Klasifikasi(len(class_names))\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b29e121107ba44959739976c3732944f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1534.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e470371265d41338de645c3631f39c4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=497810400.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNQKkdb77tOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94d1e772-d0fe-4764-fa56-bc4af5751161"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * 1\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "model = model.train()\n",
        "losses = []\n",
        "correct_predictions = 0\n",
        "for d in train_data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    print(f'loss {np.mean(losses)} acc {correct_predictions.double() / len(train_df)}')\n",
        "    #nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss 1.6153987646102905 acc 0.0007925500297206261\n",
            "loss 1.6514875888824463 acc 0.0015851000594412521\n",
            "loss 1.6425571044286091 acc 0.002575787596592035\n",
            "loss 1.5315076112747192 acc 0.004359025163463443\n",
            "loss 1.5086069583892823 acc 0.005547850208044382\n",
            "loss 1.5933138132095337 acc 0.006142262730334852\n",
            "loss 1.575071164539882 acc 0.007331087774915791\n",
            "loss 1.5608262568712234 acc 0.008321775312066574\n",
            "loss 1.5994464423921373 acc 0.009510600356647513\n",
            "loss 1.5870334267616273 acc 0.010897562908658608\n",
            "loss 1.5719547271728516 acc 0.012284525460669705\n",
            "loss 1.5556395451227825 acc 0.014067763027541112\n",
            "loss 1.5169248764331524 acc 0.016049138101842678\n",
            "loss 1.545971495764596 acc 0.016247275609272836\n",
            "loss 1.5801183541615804 acc 0.01703982563899346\n",
            "loss 1.570292942225933 acc 0.0182286506835744\n",
            "loss 1.5516383297303145 acc 0.019615613235585496\n",
            "loss 1.5355081492000155 acc 0.02120071329502675\n",
            "loss 1.5365213093004728 acc 0.022587675847037844\n",
            "loss 1.521555757522583 acc 0.02437091341390925\n",
            "loss 1.507770606449672 acc 0.025559738458490192\n",
            "loss 1.49030310457403 acc 0.027144838517931442\n",
            "loss 1.473684145056683 acc 0.028928076084802853\n",
            "loss 1.4447301998734474 acc 0.031503863681394885\n",
            "loss 1.4407491612434387 acc 0.032692688725975826\n",
            "loss 1.4376248556834001 acc 0.034277788785417075\n",
            "loss 1.4295855826801724 acc 0.03625916385971864\n",
            "loss 1.432826252920287 acc 0.03744798890429958\n",
            "loss 1.444644687504604 acc 0.038834951456310676\n",
            "loss 1.4413837810357413 acc 0.04042005151575193\n",
            "loss 1.4417583730912977 acc 0.0424014265900535\n",
            "loss 1.4227397199720144 acc 0.04438280166435506\n",
            "loss 1.414711058139801 acc 0.04636417673865662\n",
            "loss 1.4017543845316942 acc 0.0487418268278185\n",
            "loss 1.3889725242342268 acc 0.050525064394689914\n",
            "loss 1.3800017303890653 acc 0.052110164454131164\n",
            "loss 1.3778232593794126 acc 0.053298989498712104\n",
            "loss 1.3766338793854964 acc 0.0546859520507232\n",
            "loss 1.3578909054780617 acc 0.057261739647315235\n",
            "loss 1.3488944709300994 acc 0.059441252229046955\n",
            "loss 1.3522417603469477 acc 0.06082821478105805\n",
            "loss 1.3369506271112532 acc 0.06360213988508025\n",
            "loss 1.3249304419340089 acc 0.06597978997424211\n",
            "loss 1.3129612586715005 acc 0.06796116504854369\n",
            "loss 1.300490767425961 acc 0.07033881513770557\n",
            "loss 1.2834154691385187 acc 0.0729146027342976\n",
            "loss 1.2680956061850204 acc 0.07529225282345947\n",
            "loss 1.2618157602846622 acc 0.07727362789776104\n",
            "loss 1.2460651926848354 acc 0.08004755300178323\n",
            "loss 1.2435087531805038 acc 0.08202892807608479\n",
            "loss 1.2437671463863522 acc 0.08420844065781652\n",
            "loss 1.225760304583953 acc 0.08718050326926886\n",
            "loss 1.2200191004096337 acc 0.08896374083614028\n",
            "loss 1.2112837693205587 acc 0.09134139092530215\n",
            "loss 1.2086549666794864 acc 0.09352090350703388\n",
            "loss 1.1979284706924642 acc 0.09589855359619576\n",
            "loss 1.1857379335060454 acc 0.09847434119278779\n",
            "loss 1.1841264320858593 acc 0.10005944125222904\n",
            "loss 1.175071525876805 acc 0.10243709134139092\n",
            "loss 1.171423208216826 acc 0.1048147414305528\n",
            "loss 1.1653322152426986 acc 0.10719239151971467\n",
            "loss 1.1637962128846877 acc 0.10857935407172577\n",
            "loss 1.1589586701658037 acc 0.11016445413116703\n",
            "loss 1.1509776427410543 acc 0.11194769169803843\n",
            "loss 1.1502891976099747 acc 0.11392906677233999\n",
            "loss 1.1460021558133038 acc 0.11591044184664157\n",
            "loss 1.1443844717829974 acc 0.11769367941351297\n",
            "loss 1.1412410004174007 acc 0.1198731919952447\n",
            "loss 1.1341703097889388 acc 0.12185456706954625\n",
            "loss 1.12651444034917 acc 0.12423221715870814\n",
            "loss 1.1213347496281207 acc 0.1258173172181494\n",
            "loss 1.118178805957238 acc 0.1276005547850208\n",
            "loss 1.1188422061809122 acc 0.1285912423221716\n",
            "loss 1.1185864898804072 acc 0.13057261739647313\n",
            "loss 1.1247801665465038 acc 0.1321577174559144\n",
            "loss 1.1186995808231204 acc 0.13453536754507628\n",
            "loss 1.1144111291928724 acc 0.13612046760451751\n",
            "loss 1.1059103103784413 acc 0.13869625520110956\n",
            "loss 1.1004542580133752 acc 0.14087576778284128\n",
            "loss 1.097319132834673 acc 0.14325341787200316\n",
            "loss 1.0960200164053175 acc 0.14543293045373487\n",
            "loss 1.092411499924776 acc 0.1472161680206063\n",
            "loss 1.0884406351181397 acc 0.14959381810976818\n",
            "loss 1.0812107924194563 acc 0.1521696057063602\n",
            "loss 1.078895809019313 acc 0.15434911828809192\n",
            "loss 1.0759245645861293 acc 0.1563304933623935\n",
            "loss 1.0666628229892117 acc 0.15930255597384585\n",
            "loss 1.0565887896174735 acc 0.16247275609272835\n",
            "loss 1.0487608383880573 acc 0.16524668119675054\n",
            "loss 1.048992066582044 acc 0.16782246879334256\n",
            "loss 1.0394623161672236 acc 0.17079453140479492\n",
            "loss 1.03794313578502 acc 0.17257776897166632\n",
            "loss 1.0311315373707843 acc 0.1753516940756885\n",
            "loss 1.024706762521825 acc 0.1781256191797107\n",
            "loss 1.0169851767389397 acc 0.1808995442837329\n",
            "loss 1.01270775248607 acc 0.18347533188032494\n",
            "loss 1.0086863618536093 acc 0.18585298196948682\n",
            "loss 1.0013626935530682 acc 0.18882504458093916\n",
            "loss 0.9997544746206264 acc 0.19080641965524073\n",
            "loss 0.9929384469985962 acc 0.1937784822666931\n",
            "loss 0.9879876900427412 acc 0.19655240737071528\n",
            "loss 0.9879694545970243 acc 0.198731919952447\n",
            "loss 0.9900911502467776 acc 0.20110957004160887\n",
            "loss 0.9915396410685319 acc 0.20309094511591044\n",
            "loss 0.9921529236293974 acc 0.205072320190212\n",
            "loss 0.9886468255294943 acc 0.20744997027937387\n",
            "loss 0.9845189663851373 acc 0.20982762036853575\n",
            "loss 0.9810069777347423 acc 0.21200713295026746\n",
            "loss 0.9813111725203488 acc 0.21398850802456904\n",
            "loss 0.9784653766588731 acc 0.21656429562116108\n",
            "loss 0.9727751714689238 acc 0.21953635823261342\n",
            "loss 0.9682919782187257 acc 0.2219140083217753\n",
            "loss 0.9658712749987577 acc 0.22389538339607687\n",
            "loss 0.964482071106894 acc 0.22587675847037844\n",
            "loss 0.9628052644107653 acc 0.22825440855954032\n",
            "loss 0.9640872391133473 acc 0.23043392114127204\n",
            "loss 0.9633610365737197 acc 0.23261343372300375\n",
            "loss 0.9620113933490495 acc 0.23499108381216563\n",
            "loss 0.9570139751714819 acc 0.23756687140875765\n",
            "loss 0.9559906462828318 acc 0.2397463839904894\n",
            "loss 0.9529873388857881 acc 0.24252030909451158\n",
            "loss 0.9515822158485162 acc 0.24450168416881315\n",
            "loss 0.9488099585703718 acc 0.246879334257975\n",
            "loss 0.9481003438272784 acc 0.24905884683970675\n",
            "loss 0.9481186261177063 acc 0.25123835942143846\n",
            "loss 0.9418243642837282 acc 0.254408559540321\n",
            "loss 0.9364339730401677 acc 0.2573806221517733\n",
            "loss 0.9305717987008393 acc 0.26055082227065585\n",
            "loss 0.9261344738246858 acc 0.26332474737467804\n",
            "loss 0.9208401462206474 acc 0.26629680998613037\n",
            "loss 0.9161294855689275 acc 0.2692688725975827\n",
            "loss 0.9137485959764683 acc 0.2716465226867446\n",
            "loss 0.9096885256301191 acc 0.27461858529819694\n",
            "loss 0.9076865742455668 acc 0.27739251040221913\n",
            "loss 0.9041679007035714 acc 0.2801664355062413\n",
            "loss 0.9021397132207366 acc 0.2829403606102635\n",
            "loss 0.9019711813787474 acc 0.28551614820685556\n",
            "loss 0.9036417936069377 acc 0.28769566078858727\n",
            "loss 0.8997317288848136 acc 0.29046958589260946\n",
            "loss 0.8954274982213974 acc 0.2934416485040618\n",
            "loss 0.8927152558421412 acc 0.29601743610065384\n",
            "loss 0.8923873288530699 acc 0.2983950861898157\n",
            "loss 0.8948535819153686 acc 0.30057459877154746\n",
            "loss 0.893401939007971 acc 0.30315038636813946\n",
            "loss 0.8920903571720781 acc 0.3057261739647315\n",
            "loss 0.8904643136344544 acc 0.30830196156132356\n",
            "loss 0.8895817354422848 acc 0.31087774915791555\n",
            "loss 0.8900041419106561 acc 0.3130572617396473\n",
            "loss 0.8919936746558887 acc 0.3150386368139489\n",
            "loss 0.8900760781764984 acc 0.3176144244105409\n",
            "loss 0.8894198584240793 acc 0.31959579948484246\n",
            "loss 0.8865289990054933 acc 0.32236972458886465\n",
            "loss 0.8839128019373401 acc 0.32514364969288684\n",
            "loss 0.8817522249051503 acc 0.32752129978204875\n",
            "loss 0.8803012507577096 acc 0.33009708737864074\n",
            "loss 0.8785970870118874 acc 0.33287101248266293\n",
            "loss 0.8779459673507958 acc 0.3348523875569645\n",
            "loss 0.8754258267109907 acc 0.33742817515355655\n",
            "loss 0.8722150593808612 acc 0.3404002377650089\n",
            "loss 0.8708439553156495 acc 0.3427778878541708\n",
            "loss 0.8707496858901859 acc 0.3449574004359025\n",
            "loss 0.8683704713612427 acc 0.3477313255399247\n",
            "loss 0.8662954395168398 acc 0.3505052506439469\n",
            "loss 0.8639237680813161 acc 0.3532791757479691\n",
            "loss 0.8621945175257596 acc 0.3558549633445611\n",
            "loss 0.8596080336944166 acc 0.3584307509411531\n",
            "loss 0.8578067161365898 acc 0.36100653853774517\n",
            "loss 0.8550222406075114 acc 0.36378046364176736\n",
            "loss 0.8525131923207164 acc 0.36655438874578955\n",
            "loss 0.8528874390265521 acc 0.36893203883495146\n",
            "loss 0.8509815593211971 acc 0.37170596393897365\n",
            "loss 0.8475867819993995 acc 0.374678026550426\n",
            "loss 0.8448175841328726 acc 0.3776500891618783\n",
            "loss 0.8424035610481241 acc 0.3804240142659005\n",
            "loss 0.8394516949994223 acc 0.3831979393699227\n",
            "loss 0.8376567070795731 acc 0.38577372696651474\n",
            "loss 0.8353258285482051 acc 0.38854765207053693\n",
            "loss 0.8329421266745985 acc 0.391123439667129\n",
            "loss 0.8297267075024504 acc 0.3940955022785813\n",
            "loss 0.8267975616786215 acc 0.3968694273826035\n",
            "loss 0.8262652506156521 acc 0.39944521497919555\n",
            "loss 0.824136740395001 acc 0.40221914008321774\n",
            "loss 0.8222641444922797 acc 0.40519120269467007\n",
            "loss 0.8211088473706142 acc 0.40796512779869226\n",
            "loss 0.8179216019205144 acc 0.4109371904101446\n",
            "loss 0.8151632206093881 acc 0.4137111155141668\n",
            "loss 0.8136433903227516 acc 0.41628690311075883\n",
            "loss 0.8098414419813359 acc 0.41945710322964136\n",
            "loss 0.8071318552607581 acc 0.42223102833366355\n",
            "loss 0.8060591986304835 acc 0.4246086784228254\n",
            "loss 0.8025474247195958 acc 0.42777887854170793\n",
            "loss 0.8009536368772388 acc 0.4305528036457301\n",
            "loss 0.7985902647897987 acc 0.43352486625718245\n",
            "loss 0.7962871335216404 acc 0.43649692886863484\n",
            "loss 0.7935950390803508 acc 0.439270853972657\n",
            "loss 0.7919116137283189 acc 0.441846641569249\n",
            "loss 0.7899680710988601 acc 0.4446205666732712\n",
            "loss 0.7868347514157343 acc 0.4475926292847236\n",
            "loss 0.7849664689607956 acc 0.4503665543887458\n",
            "loss 0.7824409437179566 acc 0.4533386170001981\n",
            "loss 0.7804853523252022 acc 0.4561125421042203\n",
            "loss 0.778289674237223 acc 0.45908460471567264\n",
            "loss 0.7752304620343476 acc 0.462056667327125\n",
            "loss 0.7725811566354013 acc 0.46502872993857736\n",
            "loss 0.7717831214026707 acc 0.46780265504259955\n",
            "loss 0.7690234135076838 acc 0.470972855161482\n",
            "loss 0.766470114821973 acc 0.4739449177729344\n",
            "loss 0.7653996129162036 acc 0.4765207053695264\n",
            "loss 0.7644533832963003 acc 0.47909649296611845\n",
            "loss 0.7634594722872688 acc 0.4816722805627105\n",
            "loss 0.7615337546967782 acc 0.4844462056667327\n",
            "loss 0.760462649588315 acc 0.48682385575589454\n",
            "loss 0.7581505460638396 acc 0.48959778085991673\n",
            "loss 0.7568008532033903 acc 0.4923717059639389\n",
            "loss 0.7558855472609054 acc 0.4951456310679611\n",
            "loss 0.7543125000816805 acc 0.4981176936794135\n",
            "loss 0.7525091237186836 acc 0.5008916187834357\n",
            "loss 0.7496971319164705 acc 0.5040618189023182\n",
            "loss 0.7478252324747713 acc 0.5068357440063403\n",
            "loss 0.7446656686338511 acc 0.5100059441252229\n",
            "loss 0.7426074605721694 acc 0.5125817317218149\n",
            "loss 0.7417378146369178 acc 0.5149593818109768\n",
            "loss 0.7392606821429034 acc 0.5179314444224291\n",
            "loss 0.7388188373962683 acc 0.520309094511591\n",
            "loss 0.7367454428805246 acc 0.5230830196156132\n",
            "loss 0.7361286428254262 acc 0.5256588072122053\n",
            "loss 0.7334176380400615 acc 0.5288290073310877\n",
            "loss 0.7307216391751641 acc 0.5319992074499702\n",
            "loss 0.7284096333007104 acc 0.5351694075688528\n",
            "loss 0.7260448183702386 acc 0.5381414701803051\n",
            "loss 0.7235173735912744 acc 0.5413116702991876\n",
            "loss 0.7213655951079624 acc 0.5442837329106399\n",
            "loss 0.7200473814946899 acc 0.5470576580146621\n",
            "loss 0.7179336943942257 acc 0.5500297206261145\n",
            "loss 0.7195262141684269 acc 0.5522092332078462\n",
            "loss 0.7192028443944656 acc 0.5547850208044383\n",
            "loss 0.7182990670958652 acc 0.5575589459084604\n",
            "loss 0.715623887553185 acc 0.5607291460273429\n",
            "loss 0.7160687166340182 acc 0.5631067961165048\n",
            "loss 0.7155061782958607 acc 0.5654844462056667\n",
            "loss 0.7134503065859629 acc 0.5684565088171191\n",
            "loss 0.7118731998522912 acc 0.5714285714285714\n",
            "loss 0.7100999859686742 acc 0.5744006340400237\n",
            "loss 0.7098270558492571 acc 0.5767782841291856\n",
            "loss 0.7095486343211057 acc 0.5793540717257777\n",
            "loss 0.71124513934904 acc 0.5815335843075093\n",
            "loss 0.7115355835330148 acc 0.5839112343966713\n",
            "loss 0.7120258045352755 acc 0.5862888844858332\n",
            "loss 0.7106083399619922 acc 0.5890628095898554\n",
            "loss 0.7098281426727772 acc 0.5916385971864474\n",
            "loss 0.7093757987319236 acc 0.5942143847830395\n",
            "loss 0.7075903715181445 acc 0.5971864473944918\n",
            "loss 0.7059762059994366 acc 0.599960372498514\n",
            "loss 0.7051635036376986 acc 0.6023380225876758\n",
            "loss 0.7044074158165969 acc 0.605111947691698\n",
            "loss 0.7020817106822506 acc 0.6082821478105805\n",
            "loss 0.7014198241762614 acc 0.6110560729146027\n",
            "loss 0.6990541317030903 acc 0.6142262730334852\n",
            "loss 0.6978615134896919 acc 0.6168020606300773\n",
            "loss 0.6972681819532927 acc 0.6195759857340994\n",
            "loss 0.6959715920903674 acc 0.6223499108381216\n",
            "loss 0.6949584802882817 acc 0.6247275609272835\n",
            "loss 0.692935607250879 acc 0.6276996235387359\n",
            "loss 0.6913701047217755 acc 0.6304735486427581\n",
            "loss 0.6909820848759615 acc 0.6330493362393501\n",
            "loss 0.6892051817499157 acc 0.6358232613433723\n",
            "loss 0.6878866126642245 acc 0.6385971864473945\n",
            "loss 0.6869787135397765 acc 0.6411729740439865\n",
            "loss 0.6861686585505655 acc 0.6433524866257182\n",
            "loss 0.6845259043077628 acc 0.6463245492371705\n",
            "loss 0.6826934001487559 acc 0.6494947493560531\n",
            "loss 0.6814744617954335 acc 0.6524668119675054\n",
            "loss 0.6807727717381694 acc 0.6552407370715276\n",
            "loss 0.6805506972664029 acc 0.6576183871606894\n",
            "loss 0.679551123245196 acc 0.6601941747572815\n",
            "loss 0.6785152722689984 acc 0.6629680998613037\n",
            "loss 0.6768292992069833 acc 0.665940162472756\n",
            "loss 0.6751853347950273 acc 0.6689122250842084\n",
            "loss 0.6739956127967032 acc 0.6718842876956608\n",
            "loss 0.6722710882180504 acc 0.6750544878145432\n",
            "loss 0.6721526719762337 acc 0.6776302754111353\n",
            "loss 0.6725623527581387 acc 0.6802060630077273\n",
            "loss 0.6718197329358162 acc 0.6831781256191797\n",
            "loss 0.6704405693363555 acc 0.686150188230632\n",
            "loss 0.6691370978951454 acc 0.6889241133346542\n",
            "loss 0.6672413384841456 acc 0.6920943134535367\n",
            "loss 0.6653773294768267 acc 0.6950663760649891\n",
            "loss 0.6637954539878087 acc 0.6980384386764414\n",
            "loss 0.6627662445589333 acc 0.7006142262730335\n",
            "loss 0.6611101884523343 acc 0.7035862888844858\n",
            "loss 0.6612071516024288 acc 0.7059639389736476\n",
            "loss 0.6602400635458427 acc 0.7085397265702397\n",
            "loss 0.6585374356282448 acc 0.7117099266891223\n",
            "loss 0.656684103795663 acc 0.7148801268080047\n",
            "loss 0.6552084628555734 acc 0.717852189419457\n",
            "loss 0.6534563816731443 acc 0.7210223895383395\n",
            "loss 0.6526075641966428 acc 0.7237963146423617\n",
            "loss 0.6507488454898331 acc 0.7269665147612443\n",
            "loss 0.650821650083049 acc 0.7293441648504061\n",
            "loss 0.6501171334336201 acc 0.731721814939568\n",
            "loss 0.6484738010356197 acc 0.7346938775510203\n",
            "loss 0.6477578149380668 acc 0.7374678026550425\n",
            "loss 0.6477937026828429 acc 0.7400435902516346\n",
            "loss 0.6469290843968721 acc 0.7424212403407965\n",
            "loss 0.6470679214010473 acc 0.7447988904299584\n",
            "loss 0.6460008410116037 acc 0.7477709530414107\n",
            "loss 0.6460504255436531 acc 0.7503467406380028\n",
            "loss 0.6444007102883869 acc 0.7535169407568852\n",
            "loss 0.6436157655513403 acc 0.7562908658609074\n",
            "loss 0.6417907741040952 acc 0.7594610659797899\n",
            "loss 0.6402450942964416 acc 0.7624331285912422\n",
            "loss 0.6390486057991018 acc 0.7652070536952644\n",
            "loss 0.6383773524778339 acc 0.7679809787992866\n",
            "loss 0.6373892766512503 acc 0.7707549039033088\n",
            "loss 0.6360237005920637 acc 0.7737269665147612\n",
            "loss 0.6345331106995102 acc 0.7751139290667723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohi5dmgN8h6e"
      },
      "source": [
        "EPOCHS = 6\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM8k-vAN95yB"
      },
      "source": [
        "#TESTTTTTTTTTTTTTTTTTTTT\n",
        "def train_epoch(\n",
        "  model, \n",
        "  data_loader, \n",
        "  loss_fn, \n",
        "  optimizer, \n",
        "  device, \n",
        "  scheduler\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  #correct_predictions = 0\n",
        "\n",
        "  pred_labels = []\n",
        "  true_labels = []\n",
        "  \n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    #_, preds = torch.max(outputs, dim=1)\n",
        "    \n",
        "    loss = loss_fn(outputs, torch.max(targets.float(), 1)[1])\n",
        "\n",
        "    outputs = F.softmax(outputs,dim=1)\n",
        "    #correct_predictions += torch.sum(torch.max(outputs, 1)[1] == torch.max(targets, 1)[1])\n",
        "\n",
        "    b_probs = outputs.detach().cpu().numpy()\n",
        "    b_targets = targets.detach().cpu().numpy()\n",
        "\n",
        "    true_labels.append(b_targets)\n",
        "    pred_labels.append(b_probs)\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    #print(f'train acc {correct_predictions.double() / n_examples} acc1 {accuracy_score(true_bools, pred_bools)} loss {np.mean(losses)} ')\n",
        "  \n",
        "  pred_labels = [item for sublist in pred_labels for item in sublist]\n",
        "  true_labels = [item for sublist in true_labels for item in sublist]\n",
        "  \n",
        "  threshold = 0.50\n",
        "  pred_bools = [pl>threshold for pl in pred_labels]\n",
        "  true_bools = [tl==1 for tl in true_labels]\n",
        "\n",
        "  return accuracy_score(true_bools, pred_bools), np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTTB_XyM8o4R"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  #correct_predictions = 0\n",
        "\n",
        "  pred_labels = []\n",
        "  true_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      #_, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, torch.max(targets.float(), 1)[1])\n",
        "\n",
        "      outputs = F.softmax(outputs,dim=1)\n",
        "      #correct_predictions += torch.sum(torch.max(outputs, 1)[1] == torch.max(targets, 1)[1])\n",
        "\n",
        "      b_probs = outputs.detach().cpu().numpy()\n",
        "      b_targets = targets.detach().cpu().numpy()\n",
        "\n",
        "      true_labels.append(b_targets)\n",
        "      pred_labels.append(b_probs)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "      #print(f'val acc {correct_predictions.double() / n_examples} acc1 {accuracy_score(true_bools, pred_bools)} loss {np.mean(losses)} ')\n",
        "\n",
        "  pred_labels = [item for sublist in pred_labels for item in sublist]\n",
        "  true_labels = [item for sublist in true_labels for item in sublist]\n",
        "  \n",
        "  threshold = 0.50\n",
        "  pred_bools = [pl>threshold for pl in pred_labels]\n",
        "  true_bools = [tl==1 for tl in true_labels]\n",
        "\n",
        "  return accuracy_score(true_bools, pred_bools), np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY6SFBmn8t-V",
        "outputId": "354d5425-0a34-444b-86d9-02500fcf1b8a"
      },
      "source": [
        "#%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), f'softmax_layer_2e-5_0.3_{EPOCHS}.bin')\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.34686849611115794 accuracy 0.8779472954230236\n",
            "Val   loss 0.20202611499698833 accuracy 0.9413629160063391\n",
            "Epoch 2/6\n",
            "----------\n",
            "Train loss 0.16418513069717697 accuracy 0.9502674856350307\n",
            "Val   loss 0.13325684594456105 accuracy 0.9572107765451664\n",
            "Epoch 3/6\n",
            "----------\n",
            "Train loss 0.0980572825875369 accuracy 0.9724588864672082\n",
            "Val   loss 0.18222923997818724 accuracy 0.9540412044374009\n",
            "Epoch 4/6\n",
            "----------\n",
            "Train loss 0.05671000390522082 accuracy 0.984743411927878\n",
            "Val   loss 0.18400868690077915 accuracy 0.96513470681458\n",
            "Epoch 5/6\n",
            "----------\n",
            "Train loss 0.029082776750386338 accuracy 0.9938577372696652\n",
            "Val   loss 0.21059459558382515 accuracy 0.9587955625990491\n",
            "Epoch 6/6\n",
            "----------\n",
            "Train loss 0.014668625325669647 accuracy 0.9966316623736874\n",
            "Val   loss 0.2159737174260954 accuracy 0.9572107765451664\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYXMMT79aMMY"
      },
      "source": [
        "text = ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEg7kmDkFnUr",
        "outputId": "df63c8b3-f65c-409b-a62a-12dd23a60983"
      },
      "source": [
        "#%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_acc1, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(train_df)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc} accuracy1 {train_acc1}')\n",
        "\n",
        "  val_acc, val_acc1, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(val_df)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc} accuracy1 {val_acc1}')\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  #if val_acc > best_accuracy:\n",
        "  torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "  #  best_accuracy = val_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train acc 0.0005944125222904696 loss 1.7766920328140259 \n",
            "train acc 0.0007925500297206261 loss 1.8515344262123108 \n",
            "train acc 0.0017832375668714088 loss 1.7463825543721516 \n",
            "train acc 0.0037646126411729737 loss 1.6085949838161469 \n",
            "train acc 0.00515157519318407 loss 1.5281674146652222 \n",
            "train acc 0.006340400237765009 loss 1.4791565537452698 \n",
            "train acc 0.008718050326926886 loss 1.420974680355617 \n",
            "train acc 0.011690112938379234 loss 1.297740489244461 \n",
            "train acc 0.01426590053497127 loss 1.2209632131788466 \n",
            "train acc 0.016841688131563302 loss 1.1439212322235108 \n",
            "train acc 0.019219338220725184 loss 1.0935175852342085 \n",
            "train acc 0.021993263324747374 loss 1.0377427662412326 \n",
            "train acc 0.024965325936199722 loss 0.9788950154414544 \n",
            "train acc 0.027739251040221912 loss 0.9424300470522472 \n",
            "train acc 0.030513176144244102 loss 0.9014499167601268 \n",
            "train acc 0.03308896374083614 loss 0.8814006354659796 \n",
            "train acc 0.03566475133742817 loss 0.8503478818080005 \n",
            "train acc 0.0378442639191599 loss 0.8519906683100594 \n",
            "train acc 0.04042005151575193 loss 0.8485382622794101 \n",
            "train acc 0.04279770160491381 loss 0.8279704362154007 \n",
            "train acc 0.04576976421636616 loss 0.8001472900311152 \n",
            "train acc 0.0487418268278185 loss 0.7763792933388189 \n",
            "train acc 0.05072320190212007 loss 0.7889424776253493 \n",
            "train acc 0.053298989498712104 loss 0.7773575863490502 \n",
            "train acc 0.05627105211016445 loss 0.7596034413576126 \n",
            "train acc 0.058846839706756485 loss 0.7576974177589784 \n",
            "train acc 0.06142262730334852 loss 0.7563329609455886 \n",
            "train acc 0.06419655240737071 loss 0.7411236055195332 \n",
            "train acc 0.06657420249653259 loss 0.7300816152630181 \n",
            "train acc 0.06974440261541509 loss 0.709626633922259 \n",
            "train acc 0.07271646522686744 loss 0.6958188318437145 \n",
            "train acc 0.0756885278383198 loss 0.6859304597601295 \n",
            "train acc 0.07826431543491183 loss 0.6913705423022761 \n",
            "train acc 0.08123637804636417 loss 0.6822319109650219 \n",
            "train acc 0.08401030315038636 loss 0.6708206849438804 \n",
            "train acc 0.08718050326926886 loss 0.6541505244870981 \n",
            "train acc 0.09015256588072122 loss 0.6399652804877307 \n",
            "train acc 0.09292649098474341 loss 0.6390120661572406 \n",
            "train acc 0.09609669110362591 loss 0.6248420152144555 \n",
            "train acc 0.09926689122250841 loss 0.6108813991770148 \n",
            "train acc 0.10184267881910045 loss 0.6062534667733239 \n",
            "train acc 0.10461660392312264 loss 0.5960025342092627 \n",
            "train acc 0.107588666534575 loss 0.5889844563464786 \n",
            "train acc 0.11016445413116703 loss 0.5855422245169227 \n",
            "train acc 0.11293837923518922 loss 0.5790638170308537 \n",
            "train acc 0.11551416683178126 loss 0.5748029858197855 \n",
            "train acc 0.11808995442837328 loss 0.5711514069995982 \n",
            "train acc 0.12086387953239548 loss 0.5773457358591259 \n",
            "train acc 0.12383594214384783 loss 0.5684375162331425 \n",
            "train acc 0.12641172974043985 loss 0.5624917788803577 \n",
            "train acc 0.1293837923518922 loss 0.5563178680398885 \n",
            "train acc 0.1325539924707747 loss 0.5464857583865523 \n",
            "train acc 0.1353279175747969 loss 0.5410030413208142 \n",
            "train acc 0.13790370517138895 loss 0.5386009792348853 \n",
            "train acc 0.14067763027541114 loss 0.5365834053944457 \n",
            "train acc 0.14325341787200316 loss 0.5393361461881016 \n",
            "train acc 0.14602734297602535 loss 0.5411168056491175 \n",
            "train acc 0.14840499306518723 loss 0.5548815431137537 \n",
            "train acc 0.1513770556766396 loss 0.5479340072784383 \n",
            "train acc 0.15415098078066178 loss 0.5435138681903482 \n",
            "train acc 0.15732118089954428 loss 0.5353718937420454 \n",
            "train acc 0.16009510600356647 loss 0.5340357374760413 \n",
            "train acc 0.16286903110758866 loss 0.5329877298975748 \n",
            "train acc 0.16584109371904102 loss 0.5294299274682999 \n",
            "train acc 0.16881315633049335 loss 0.5249526649713516 \n",
            "train acc 0.1717852189419457 loss 0.5187966316712626 \n",
            "train acc 0.17396473152367742 loss 0.5234149328585881 \n",
            "train acc 0.17693679413512978 loss 0.518191472024602 \n",
            "train acc 0.17971071923915197 loss 0.5179888505650603 \n",
            "train acc 0.18248464434317416 loss 0.5179414701248918 \n",
            "train acc 0.18565484446205666 loss 0.5122626033467306 \n",
            "train acc 0.18862690707350901 loss 0.5121526954074701 \n",
            "train acc 0.19159896968496135 loss 0.5078571165261203 \n",
            "train acc 0.19437289478898354 loss 0.5049911999219173 \n",
            "train acc 0.19694868238557559 loss 0.5078031448523204 \n",
            "train acc 0.19992074499702792 loss 0.5027936787197465 \n",
            "train acc 0.20289280760848027 loss 0.4989318460613102 \n",
            "train acc 0.2054685952050723 loss 0.4984482488571069 \n",
            "train acc 0.20844065781652465 loss 0.49587189962592304 \n",
            "train acc 0.21121458292054685 loss 0.4945263653993607 \n",
            "train acc 0.21398850802456904 loss 0.4943729536032971 \n",
            "train acc 0.21636615811373092 loss 0.49427006230121706 \n",
            "train acc 0.2191400832177531 loss 0.49507312602307424 \n",
            "train acc 0.22211214582920547 loss 0.492993698943229 \n",
            "train acc 0.22488607093322766 loss 0.49213287409614115 \n",
            "train acc 0.22726372102238954 loss 0.492530131062796 \n",
            "train acc 0.23023578363384187 loss 0.4904374415161966 \n",
            "train acc 0.23340598375272437 loss 0.48594884582879866 \n",
            "train acc 0.23617990885674656 loss 0.4825390558061975 \n",
            "train acc 0.2387556964533386 loss 0.4850945877532164 \n",
            "train acc 0.24172775906479096 loss 0.48108299657866194 \n",
            "train acc 0.2446998216762433 loss 0.47709319473284745 \n",
            "train acc 0.2478700217951258 loss 0.47287980010432584 \n",
            "train acc 0.2510402219140083 loss 0.4684026074853349 \n",
            "train acc 0.25401228452546065 loss 0.46445988895077456 \n",
            "train acc 0.2571824846443432 loss 0.45977087965002283 \n",
            "train acc 0.2601545472557955 loss 0.4566306176297751 \n",
            "train acc 0.26312660986724784 loss 0.4536470227437664 \n",
            "train acc 0.2660986724787002 loss 0.45070309404547165 \n",
            "train acc 0.2692688725975827 loss 0.4465142301656306 \n",
            "train acc 0.27224093520903503 loss 0.44342033352961047 \n",
            "train acc 0.2748167228056271 loss 0.44524195331934036 \n",
            "train acc 0.27759064790964927 loss 0.4440510314391944 \n",
            "train acc 0.28036457301367146 loss 0.44589747268205077 \n",
            "train acc 0.28254408559540317 loss 0.4526891028065057 \n",
            "train acc 0.2857142857142857 loss 0.44857844497727334 \n",
            "train acc 0.28829007331087775 loss 0.45116468134257837 \n",
            "train acc 0.29086586090746974 loss 0.451576045938526 \n",
            "train acc 0.29363978601149193 loss 0.4538224985942655 \n",
            "train acc 0.2966118486229443 loss 0.45267077696255664 \n",
            "train acc 0.29958391123439665 loss 0.4505541877535818 \n",
            "train acc 0.3027541113532792 loss 0.44678710356155144 \n",
            "train acc 0.30513176144244103 loss 0.4480045491384458 \n",
            "train acc 0.3077075490390331 loss 0.45268275087143767 \n",
            "train acc 0.31087774915791555 loss 0.4489177725399318 \n",
            "train acc 0.3134535367545076 loss 0.44989962524188487 \n",
            "train acc 0.31642559936596 loss 0.4489135416941001 \n",
            "train acc 0.319001386962552 loss 0.45032843753253504 \n",
            "train acc 0.32177531206657417 loss 0.44886823244267654 \n",
            "train acc 0.3241529621557361 loss 0.44938156140657765 \n",
            "train acc 0.32692688725975827 loss 0.4488185654771476 \n",
            "train acc 0.3298989498712106 loss 0.44857718082541814 \n",
            "train acc 0.33247473746780265 loss 0.4497992809013865 \n",
            "train acc 0.335446800079255 loss 0.4472216929669582 \n",
            "train acc 0.3378244501684169 loss 0.45145970244705674 \n",
            "train acc 0.34099465028729936 loss 0.44801691317901254 \n",
            "train acc 0.3439667128987517 loss 0.44514457783185124 \n",
            "train acc 0.3471369130176342 loss 0.44217411418503616 \n",
            "train acc 0.3499108381216564 loss 0.4419070499725351 \n",
            "train acc 0.35288290073310874 loss 0.4391951288884649 \n",
            "train acc 0.3558549633445611 loss 0.4367278860526458 \n",
            "train acc 0.3584307509411531 loss 0.43536961266319407 \n",
            "train acc 0.36160095106003565 loss 0.43271068288923653 \n",
            "train acc 0.36437487616405784 loss 0.4319260515300418 \n",
            "train acc 0.36754507628294036 loss 0.4293865178056337 \n",
            "train acc 0.3705171388943927 loss 0.427235524293364 \n",
            "train acc 0.37368733901327517 loss 0.42443100024476976 \n",
            "train acc 0.37586685159500693 loss 0.4259488141380142 \n",
            "train acc 0.37883891420645927 loss 0.4251038787363888 \n",
            "train acc 0.3818109768179116 loss 0.42291701514539976 \n",
            "train acc 0.3845849019219338 loss 0.4223163767671543 \n",
            "train acc 0.3877551020408163 loss 0.4201043354557224 \n",
            "train acc 0.3905290271448385 loss 0.4205990635197271 \n",
            "train acc 0.3933029522488607 loss 0.421247511002649 \n",
            "train acc 0.39568060233802255 loss 0.4215864826250693 \n",
            "train acc 0.3982563899346146 loss 0.42065019021448613 \n",
            "train acc 0.40083217753120665 loss 0.42130739950150453 \n",
            "train acc 0.4034079651277987 loss 0.4217242672410164 \n",
            "train acc 0.4061818902318209 loss 0.42180115058268075 \n",
            "train acc 0.4089558153358431 loss 0.4232862523570657 \n",
            "train acc 0.4119278779472954 loss 0.4225496603731091 \n",
            "train acc 0.4150980780661779 loss 0.42009724712146346 \n",
            "train acc 0.41807014067763026 loss 0.41827486052476115 \n",
            "train acc 0.4210422032890826 loss 0.4161365417908732 \n",
            "train acc 0.4240142659005349 loss 0.41428146435608787 \n",
            "train acc 0.4267881910045571 loss 0.4130543901776083 \n",
            "train acc 0.4297602536160095 loss 0.4125730753727969 \n",
            "train acc 0.432930453734892 loss 0.41034059890227603 \n",
            "train acc 0.4351099663166237 loss 0.4133834895449427 \n",
            "train acc 0.4380820289280761 loss 0.41163634177064523 \n",
            "train acc 0.4410540915395284 loss 0.41199400672770065 \n",
            "train acc 0.4438280166435506 loss 0.41104926393125896 \n",
            "train acc 0.4468000792550029 loss 0.4097555793844666 \n",
            "train acc 0.4495740043590251 loss 0.4082785786084104 \n",
            "train acc 0.45274420447790764 loss 0.4059259586036205 \n",
            "train acc 0.45551812958192983 loss 0.40621408530656833 \n",
            "train acc 0.45849019219338216 loss 0.4054041313687841 \n",
            "train acc 0.46126411729740435 loss 0.40446359546677696 \n",
            "train acc 0.4644343174162869 loss 0.4023081736099085 \n",
            "train acc 0.46720824252030907 loss 0.40120019939015894 \n",
            "train acc 0.4703784426391916 loss 0.39924334329471256 \n",
            "train acc 0.47354864275807407 loss 0.3975025117830482 \n",
            "train acc 0.475926292847236 loss 0.3998872993050972 \n",
            "train acc 0.47870021795125817 loss 0.3991846836589534 \n",
            "train acc 0.48187041807014064 loss 0.39728246514286314 \n",
            "train acc 0.48464434317416283 loss 0.397223174021664 \n",
            "train acc 0.4876164057856152 loss 0.3958600199239402 \n",
            "train acc 0.4903903308896374 loss 0.39703107403402915 \n",
            "train acc 0.49336239350108974 loss 0.3965394077770537 \n",
            "train acc 0.49633445611254207 loss 0.39622562184102006 \n",
            "train acc 0.4989102437091341 loss 0.3968372294339686 \n",
            "train acc 0.5020804438280166 loss 0.39503517366208873 \n",
            "train acc 0.505052506439469 loss 0.3946226826701008 \n",
            "train acc 0.507628294036061 loss 0.3961765628065104 \n",
            "train acc 0.5102040816326531 loss 0.3949694727320929 \n",
            "train acc 0.5125817317218149 loss 0.39482970459647077 \n",
            "train acc 0.5153556568258371 loss 0.3941574948117695 \n",
            "train acc 0.5185258569447196 loss 0.3921328201949438 \n",
            "train acc 0.521497919556172 loss 0.39160290477728404 \n",
            "train acc 0.5242718446601942 loss 0.3926326128114995 \n",
            "train acc 0.5274420447790766 loss 0.3906558209406066 \n",
            "train acc 0.5300178323756687 loss 0.38996577126575477 \n",
            "train acc 0.532989894987121 loss 0.3900276890719416 \n",
            "train acc 0.5359619575985733 loss 0.3895891639403999 \n",
            "train acc 0.5387358827025955 loss 0.38915934428477134 \n",
            "train acc 0.5417079453140479 loss 0.38818758980333046 \n",
            "train acc 0.5446800079255003 loss 0.3884150391482384 \n",
            "train acc 0.5474539330295225 loss 0.3872197750871154 \n",
            "train acc 0.5504259956409748 loss 0.38567021645923805 \n",
            "train acc 0.5533980582524272 loss 0.38453536718618125 \n",
            "train acc 0.5565682583713096 loss 0.38287878892296434 \n",
            "train acc 0.559540320982762 loss 0.3815653696610122 \n",
            "train acc 0.5627105211016445 loss 0.37981921077013164 \n",
            "train acc 0.5650881711908063 loss 0.3816414661933759 \n",
            "train acc 0.5680602338022588 loss 0.3823655677505019 \n",
            "train acc 0.570834158906281 loss 0.3819423071470437 \n",
            "train acc 0.5738062215177333 loss 0.38093191869826853 \n",
            "train acc 0.5765801466217555 loss 0.38150823770467046 \n",
            "train acc 0.5793540717257777 loss 0.3810464471231023 \n",
            "train acc 0.5821279968297999 loss 0.38221927081633894 \n",
            "train acc 0.5851000594412522 loss 0.3810285943035054 \n",
            "train acc 0.5882702595601347 loss 0.3793895949284493 \n",
            "train acc 0.5910441846641569 loss 0.3806379162203608 \n",
            "train acc 0.5940162472756092 loss 0.37946556171263074 \n",
            "train acc 0.5969883098870616 loss 0.3789585913033333 \n",
            "train acc 0.5997622349910838 loss 0.37922851627916787 \n",
            "train acc 0.6027342976025362 loss 0.37811433689128005 \n",
            "train acc 0.6059044977214186 loss 0.37664691073854173 \n",
            "train acc 0.6086784228254408 loss 0.37700693420300335 \n",
            "train acc 0.6118486229443233 loss 0.37535468503748154 \n",
            "train acc 0.6146225480483455 loss 0.3746208637000411 \n",
            "train acc 0.6171983356449375 loss 0.37435779225097987 \n",
            "train acc 0.6201703982563899 loss 0.3741671840697861 \n",
            "train acc 0.6229443233604121 loss 0.3747316845817425 \n",
            "train acc 0.6257182484644342 loss 0.37418803421573504 \n",
            "train acc 0.6284921735684564 loss 0.3751766744432394 \n",
            "train acc 0.631662373687339 loss 0.37376598885389545 \n",
            "train acc 0.6344362987913612 loss 0.37310017648218363 \n",
            "train acc 0.6374083614028135 loss 0.37200194756260474 \n",
            "train acc 0.6403804240142659 loss 0.3708203130725609 \n",
            "train acc 0.6435506241331483 loss 0.36934721519469055 \n",
            "train acc 0.6463245492371705 loss 0.37002238269558113 \n",
            "train acc 0.6492966118486229 loss 0.3703239258066971 \n",
            "train acc 0.652070536952645 loss 0.3698846366471396 \n",
            "train acc 0.6546463245492371 loss 0.3721513463897591 \n",
            "train acc 0.6572221121458292 loss 0.373649968667019 \n",
            "train acc 0.6599960372498513 loss 0.37316513826622616 \n",
            "train acc 0.6631662373687338 loss 0.3717732212365362 \n",
            "train acc 0.6657420249653259 loss 0.37240081683783227 \n",
            "train acc 0.6681196750544878 loss 0.3730583040160127 \n",
            "train acc 0.6710917376659401 loss 0.37196405672108857 \n",
            "train acc 0.6740638002773924 loss 0.37099476239821016 \n",
            "train acc 0.677234000396275 loss 0.3699145945126321 \n",
            "train acc 0.6800079255002972 loss 0.36990200265188566 \n",
            "train acc 0.6831781256191797 loss 0.36867867314009645 \n",
            "train acc 0.6859520507232019 loss 0.3686521314500975 \n",
            "train acc 0.6883297008123638 loss 0.36842573854529304 \n",
            "train acc 0.691103625916386 loss 0.36937669719824745 \n",
            "train acc 0.6942738260352684 loss 0.3683089434793855 \n",
            "train acc 0.6968496136318605 loss 0.3690076971463859 \n",
            "train acc 0.6996235387358827 loss 0.3691358880151019 \n",
            "train acc 0.7027937388547651 loss 0.3678602396015314 \n",
            "train acc 0.7059639389736476 loss 0.36662266537061206 \n",
            "train acc 0.7091341390925302 loss 0.36539609374947785 \n",
            "train acc 0.7117099266891223 loss 0.3661688174358478 \n",
            "train acc 0.7144838517931444 loss 0.36623264485024265 \n",
            "train acc 0.7172577768971666 loss 0.36625678839836023 \n",
            "train acc 0.720229839508619 loss 0.3653970445333005 \n",
            "train acc 0.7232019021200713 loss 0.3650149619278703 \n",
            "train acc 0.7259758272240935 loss 0.36467107926638653 \n",
            "train acc 0.7287497523281157 loss 0.36420589442439805 \n",
            "train acc 0.7315236774321379 loss 0.3650940507330455 \n",
            "train acc 0.73429760253616 loss 0.36475274904131777 \n",
            "train acc 0.7372696651476124 loss 0.3640269471974479 \n",
            "train acc 0.7402417277590647 loss 0.3637619988781945 \n",
            "train acc 0.7434119278779473 loss 0.36263762044377235 \n",
            "train acc 0.7463839904893996 loss 0.3624732324658206 \n",
            "train acc 0.7491579155934218 loss 0.3622989456565467 \n",
            "train acc 0.751931840697444 loss 0.36164060916031493 \n",
            "train acc 0.7549039033088963 loss 0.3610851681308338 \n",
            "train acc 0.7580741034277788 loss 0.3599249748306906 \n",
            "train acc 0.7610461660392311 loss 0.35980889269301447 \n",
            "train acc 0.7638200911432533 loss 0.35960440334587634 \n",
            "train acc 0.7669902912621359 loss 0.35850371091021566 \n",
            "train acc 0.769566078858728 loss 0.3603193645822731 \n",
            "train acc 0.7723400039627502 loss 0.3617136697318621 \n",
            "train acc 0.7751139290667723 loss 0.36157508282871775 \n",
            "train acc 0.7782841291856548 loss 0.36041385875746823 \n",
            "train acc 0.7812561917971071 loss 0.3599125146565418 \n",
            "train acc 0.7844263919159896 loss 0.35876578073616006 \n",
            "train acc 0.7872003170200118 loss 0.35949443118626856 \n",
            "train acc 0.7901723796314641 loss 0.35860190727807423 \n",
            "train acc 0.7931444422429166 loss 0.3582945670570533 \n",
            "train acc 0.7961165048543689 loss 0.35818822789137106 \n",
            "train acc 0.7988904299583911 loss 0.35807109676105414 \n",
            "train acc 0.8020606300772736 loss 0.35693715444694346 \n",
            "train acc 0.805230830196156 loss 0.3557502309457506 \n",
            "train acc 0.8084010303150386 loss 0.35459382505117293 \n",
            "train acc 0.8111749554190608 loss 0.35468126987155435 \n",
            "train acc 0.8143451555379433 loss 0.353546612873545 \n",
            "train acc 0.8171190806419655 loss 0.3530446462864472 \n",
            "train acc 0.8200911432534178 loss 0.3525095683401323 \n",
            "train acc 0.82286506835744 loss 0.3519906463012506 \n",
            "train acc 0.8260352684763225 loss 0.35086886187204397 \n",
            "train acc 0.8290073310877749 loss 0.35017440099647995 \n",
            "train acc 0.8321775312066574 loss 0.34902450815787084 \n",
            "train acc 0.8353477313255399 loss 0.34789918464066005 \n",
            "train acc 0.8385179314444224 loss 0.3467956968089378 \n",
            "train acc 0.8414899940558748 loss 0.3461677486569088 \n",
            "train acc 0.8446601941747572 loss 0.3451553481630981 \n",
            "train acc 0.8474341192787794 loss 0.345427889940004 \n",
            "train acc 0.8502080443828016 loss 0.34637135798296587 \n",
            "train acc 0.8529819694868238 loss 0.3465209363132912 \n",
            "train acc 0.8559540320982761 loss 0.346544232691246 \n",
            "train acc 0.8589260947097285 loss 0.3460041984976804 \n",
            "train acc 0.862096294828611 loss 0.34493849040587354 \n",
            "train acc 0.8648702199326332 loss 0.344356173788977 \n",
            "train acc 0.8680404200515157 loss 0.34332132800300785 \n",
            "train acc 0.871012482662968 loss 0.3428189093458035 \n",
            "train acc 0.8741826827818505 loss 0.34173715317231274 \n",
            "train acc 0.877154745393303 loss 0.3408649995087547 \n",
            "train acc 0.8801268080047553 loss 0.3402798312957972 \n",
            "train acc 0.8832970081236378 loss 0.33926321906052503 \n",
            "train acc 0.8862690707350901 loss 0.33863451490386326 \n",
            "train acc 0.8890429958391123 loss 0.33803302269162877 \n",
            "train acc 0.8904299583911234 loss 0.3369872082552366 \n",
            "Train loss 0.3369872082552366 accuracy 0.8904299583911234 accuracy1 0.8805230830196156\n",
            "val acc 0.025356576862123614 loss 0.04437285289168358 \n",
            "val acc 0.05071315372424723 loss 0.02915756031870842 \n",
            "val acc 0.07606973058637084 loss 0.038523346185684204 \n",
            "val acc 0.10142630744849446 loss 0.035870864521712065 \n",
            "val acc 0.12678288431061807 loss 0.0371185939759016 \n",
            "val acc 0.15213946117274169 loss 0.036882802533606686 \n",
            "val acc 0.1774960380348653 loss 0.03282008972018957 \n",
            "val acc 0.19968304278922347 loss 0.06142408505547792 \n",
            "val acc 0.2202852614896989 loss 0.11189750923464696 \n",
            "val acc 0.24247226624405707 loss 0.11695585930719972 \n",
            "val acc 0.26782884310618066 loss 0.10884963932701132 \n",
            "val acc 0.2916006339144216 loss 0.1287954351088653 \n",
            "val acc 0.31378763866877973 loss 0.12822772455043518 \n",
            "val acc 0.3375594294770206 loss 0.14509777731395193 \n",
            "val acc 0.3581616481774961 loss 0.1601841013257702 \n",
            "val acc 0.3835182250396197 loss 0.1587512251571752 \n",
            "val acc 0.4088748019017433 loss 0.150523308719344 \n",
            "val acc 0.4326465927099842 loss 0.1472886968921456 \n",
            "val acc 0.45641838351822506 loss 0.14591210869778143 \n",
            "val acc 0.4786053882725832 loss 0.15871390202082694 \n",
            "val acc 0.5023771790808241 loss 0.1647388632187531 \n",
            "val acc 0.5277337559429477 loss 0.15749578971669756 \n",
            "val acc 0.5530903328050714 loss 0.1508625942768286 \n",
            "val acc 0.5784469096671949 loss 0.14494622741282606 \n",
            "val acc 0.6006339144215531 loss 0.15136526817455886 \n",
            "val acc 0.6259904912836767 loss 0.14651328305570552 \n",
            "val acc 0.6497622820919177 loss 0.1438662912869067 \n",
            "val acc 0.6719492868462758 loss 0.15363783237989992 \n",
            "val acc 0.694136291600634 loss 0.1762506751953785 \n",
            "val acc 0.7179080824088748 loss 0.18191550431462625 \n",
            "val acc 0.7432646592709985 loss 0.1770695256399772 \n",
            "val acc 0.768621236133122 loss 0.172221386586898 \n",
            "val acc 0.7939778129952457 loss 0.16857230314759142 \n",
            "val acc 0.8193343898573693 loss 0.16458891202038264 \n",
            "val acc 0.844690966719493 loss 0.1602113200750734 \n",
            "val acc 0.8684627575277338 loss 0.16329935903195292 \n",
            "val acc 0.8938193343898574 loss 0.160399006390189 \n",
            "val acc 0.9160063391442156 loss 0.16604937648547716 \n",
            "val acc 0.9413629160063391 loss 0.16236121671942946 \n",
            "val acc 0.9508716323296356 loss 0.16710237773368136 \n",
            "Val   loss 0.16710237773368136 accuracy 0.9508716323296356 accuracy1 0.9477020602218701\n",
            "Epoch 2/5\n",
            "----------\n",
            "train acc 0.0029720626114523477 loss 0.25782710313796997 \n",
            "train acc 0.005944125222904695 loss 0.222116120159626 \n",
            "train acc 0.008916187834357043 loss 0.18034889300664267 \n",
            "train acc 0.01188825044580939 loss 0.15511606633663177 \n",
            "train acc 0.014464038042401426 loss 0.24528172016143798 \n",
            "train acc 0.01763423816128393 loss 0.20594886038452387 \n",
            "train acc 0.02060630077273628 loss 0.1937957318233592 \n",
            "train acc 0.02377650089161878 loss 0.17045713745756075 \n",
            "train acc 0.02674856350307113 loss 0.19996678637754586 \n",
            "train acc 0.02952248860709332 loss 0.22009653872810303 \n",
            "train acc 0.03249455121854567 loss 0.21175253099169244 \n",
            "train acc 0.03566475133742817 loss 0.1991122335117931 \n",
            "train acc 0.038834951456310676 loss 0.1843639323129677 \n",
            "train acc 0.04180701406776303 loss 0.18549809262289532 \n",
            "train acc 0.04497721418664553 loss 0.1737127224293848 \n",
            "train acc 0.04735486427580741 loss 0.20281720531056635 \n",
            "train acc 0.05032692688725975 loss 0.20476375166874597 \n",
            "train acc 0.05310085199128195 loss 0.2072087774415397 \n",
            "train acc 0.05627105211016445 loss 0.19693064594053125 \n",
            "train acc 0.059441252229046955 loss 0.18900060488376766 \n",
            "train acc 0.06241331484049931 loss 0.18288364547437855 \n",
            "train acc 0.0655835149593818 loss 0.17481051563200625 \n",
            "train acc 0.068357440063404 loss 0.17677917857856854 \n",
            "train acc 0.07113136516742619 loss 0.18280774890445173 \n",
            "train acc 0.0743015652863087 loss 0.17666354909539222 \n",
            "train acc 0.07727362789776104 loss 0.1806105181861382 \n",
            "train acc 0.08004755300178323 loss 0.18112305593159464 \n",
            "train acc 0.08301961561323558 loss 0.18011577281036548 \n",
            "train acc 0.08599167822468792 loss 0.1792799492591414 \n",
            "train acc 0.08896374083614028 loss 0.17796990883847078 \n",
            "train acc 0.09193580344759263 loss 0.17574147844026167 \n",
            "train acc 0.09510600356647513 loss 0.17060762844630517 \n",
            "train acc 0.09807806617792747 loss 0.1704613468468641 \n",
            "train acc 0.10105012878937983 loss 0.17298125182552374 \n",
            "train acc 0.10402219140083217 loss 0.1701857744849154 \n",
            "train acc 0.10719239151971467 loss 0.16590848862607446 \n",
            "train acc 0.11036259163859719 loss 0.1616593729201201 \n",
            "train acc 0.11333465425004953 loss 0.1645294789616999 \n",
            "train acc 0.11650485436893203 loss 0.16060157077243695 \n",
            "train acc 0.11967505448781454 loss 0.15684809139929712 \n",
            "train acc 0.12225084208440658 loss 0.1643367400165738 \n",
            "train acc 0.12542104220328906 loss 0.16165117808573304 \n",
            "train acc 0.1285912423221716 loss 0.15796573415160353 \n",
            "train acc 0.1317614424410541 loss 0.15447277321205052 \n",
            "train acc 0.1349316425599366 loss 0.15185919155159758 \n",
            "train acc 0.13790370517138895 loss 0.15085657345353748 \n",
            "train acc 0.14087576778284128 loss 0.15218991119949582 \n",
            "train acc 0.14345155537943333 loss 0.15794935899612028 \n",
            "train acc 0.14662175549831583 loss 0.1549267922005407 \n",
            "train acc 0.14979195561719832 loss 0.1523611219925806 \n",
            "train acc 0.15276401822865068 loss 0.15414335585528 \n",
            "train acc 0.15593421834753318 loss 0.15130489508066183 \n",
            "train acc 0.15890628095898554 loss 0.15346911509591593 \n",
            "train acc 0.16187834357043787 loss 0.15180230791318333 \n",
            "train acc 0.16485040618189023 loss 0.1516975840595974 \n",
            "train acc 0.16762433128591242 loss 0.15592318336296426 \n",
            "train acc 0.17020011888250444 loss 0.16890591267381974 \n",
            "train acc 0.17257776897166632 loss 0.18183133200790477 \n",
            "train acc 0.17574796909054882 loss 0.17885723247950486 \n",
            "train acc 0.17891816920943135 loss 0.17670962151217584 \n",
            "train acc 0.18208836932831385 loss 0.17385315588965525 \n",
            "train acc 0.18525856944719635 loss 0.1712246549601156 \n",
            "train acc 0.1882306320586487 loss 0.17375036202637212 \n",
            "train acc 0.19120269467010104 loss 0.17254129684442887 \n",
            "train acc 0.1941747572815534 loss 0.17150685527147008 \n",
            "train acc 0.1973449574004359 loss 0.1690100482735557 \n",
            "train acc 0.20031702001188825 loss 0.17191331882947194 \n",
            "train acc 0.20348722013077075 loss 0.1703655105563538 \n",
            "train acc 0.20645928274222308 loss 0.17106850957497954 \n",
            "train acc 0.20943134535367544 loss 0.17466395682921368 \n",
            "train acc 0.21260154547255794 loss 0.1722949707264107 \n",
            "train acc 0.2155736080840103 loss 0.1732675023053566 \n",
            "train acc 0.21854567069546263 loss 0.17220072322588872 \n",
            "train acc 0.22171587081434516 loss 0.17035530776263694 \n",
            "train acc 0.22488607093322766 loss 0.16874399882430832 \n",
            "train acc 0.22805627105211015 loss 0.16662196770285895 \n",
            "train acc 0.23122647117099265 loss 0.16463346171567772 \n",
            "train acc 0.23400039627501484 loss 0.17075226199813187 \n",
            "train acc 0.23717059639389734 loss 0.1689129782239376 \n",
            "train acc 0.23994452149791953 loss 0.1725143672141712 \n",
            "train acc 0.24311472161680206 loss 0.17106074933537546 \n",
            "train acc 0.2460867842282544 loss 0.17244349412688212 \n",
            "train acc 0.24886070933227658 loss 0.17544587721195387 \n",
            "train acc 0.2516346344362988 loss 0.17895686477311842 \n",
            "train acc 0.2542104220328908 loss 0.1825165768920937 \n",
            "train acc 0.2571824846443432 loss 0.1845133757857649 \n",
            "train acc 0.26035268476322565 loss 0.18244923823031373 \n",
            "train acc 0.2635228848821082 loss 0.1804426795781844 \n",
            "train acc 0.26669308500099065 loss 0.17851374253337637 \n",
            "train acc 0.2698632851198732 loss 0.17708730429617894 \n",
            "train acc 0.2730334852387557 loss 0.1755001041117114 \n",
            "train acc 0.2762036853576382 loss 0.17368240899446866 \n",
            "train acc 0.2793738854765207 loss 0.17187853035847506 \n",
            "train acc 0.28254408559540317 loss 0.17012284613372955 \n",
            "train acc 0.2857142857142857 loss 0.16844543915143922 \n",
            "train acc 0.2888844858331682 loss 0.16675124414420375 \n",
            "train acc 0.2920546859520507 loss 0.16524686380145476 \n",
            "train acc 0.2952248860709332 loss 0.16360275548103512 \n",
            "train acc 0.2983950861898157 loss 0.1623434793730878 \n",
            "train acc 0.3015652863086982 loss 0.16081885125488043 \n",
            "train acc 0.30473548642758075 loss 0.15925945962622468 \n",
            "train acc 0.30750941153160294 loss 0.16164252411701954 \n",
            "train acc 0.31048147414305527 loss 0.16354192042433954 \n",
            "train acc 0.3134535367545076 loss 0.16531318011962307 \n",
            "train acc 0.31602932435109965 loss 0.17170934948094543 \n",
            "train acc 0.3191995244699822 loss 0.17030594251470044 \n",
            "train acc 0.32177531206657417 loss 0.17579357193295506 \n",
            "train acc 0.3249455121854567 loss 0.17445948271132591 \n",
            "train acc 0.3277194372894789 loss 0.17606323704106408 \n",
            "train acc 0.3306914999009312 loss 0.17710450690832327 \n",
            "train acc 0.33366356251238355 loss 0.17770209176432308 \n",
            "train acc 0.33663562512383594 loss 0.17801556875513466 \n",
            "train acc 0.33960768773528827 loss 0.17716754187903969 \n",
            "train acc 0.3421834753318803 loss 0.1809553239257647 \n",
            "train acc 0.3453536754507628 loss 0.17944698521219518 \n",
            "train acc 0.3483257380622152 loss 0.1814529801325487 \n",
            "train acc 0.35149593818109764 loss 0.1800931392149984 \n",
            "train acc 0.35426986328511983 loss 0.18297009756867538 \n",
            "train acc 0.3572419258965722 loss 0.18374537400697835 \n",
            "train acc 0.3600158510005944 loss 0.18506273285408195 \n",
            "train acc 0.3627897761046166 loss 0.18613874597844382 \n",
            "train acc 0.3659599762234991 loss 0.1847280348215985 \n",
            "train acc 0.36893203883495146 loss 0.1866044740162305 \n",
            "train acc 0.37170596393897365 loss 0.18645786193965544 \n",
            "train acc 0.37447988904299584 loss 0.18857942498102784 \n",
            "train acc 0.3776500891618783 loss 0.18721335705384493 \n",
            "train acc 0.38082028928076084 loss 0.18579566656193863 \n",
            "train acc 0.3839904893996433 loss 0.18447851711243857 \n",
            "train acc 0.3867644145036655 loss 0.18562290599468603 \n",
            "train acc 0.389934614622548 loss 0.18427257873117925 \n",
            "train acc 0.39310481474143055 loss 0.18317194103853393 \n",
            "train acc 0.39587873984545274 loss 0.18501293583011086 \n",
            "train acc 0.3990489399643352 loss 0.1836944312291374 \n",
            "train acc 0.40221914008321774 loss 0.18240519062574231 \n",
            "train acc 0.40519120269467007 loss 0.18293005090068887 \n",
            "train acc 0.4083614028135526 loss 0.18186051472473672 \n",
            "train acc 0.41153160293243507 loss 0.18062433765616512 \n",
            "train acc 0.4141073905290271 loss 0.18147970122568632 \n",
            "train acc 0.41707945314047945 loss 0.18091124882452564 \n",
            "train acc 0.42005151575193184 loss 0.18033187332829195 \n",
            "train acc 0.4232217158708143 loss 0.1792709482833743 \n",
            "train acc 0.42639191598969683 loss 0.17810726718662281 \n",
            "train acc 0.42936397860114917 loss 0.1787366075832006 \n",
            "train acc 0.4325341787200317 loss 0.17789063558060056 \n",
            "train acc 0.43570437883891416 loss 0.17670988272875549 \n",
            "train acc 0.43867644145036655 loss 0.17657985677942634 \n",
            "train acc 0.441846641569249 loss 0.17569097201815065 \n",
            "train acc 0.44501684168813155 loss 0.1747431838066896 \n",
            "train acc 0.4479889042995839 loss 0.17634262878997814 \n",
            "train acc 0.45076282940360607 loss 0.17823501750205953 \n",
            "train acc 0.4537348920150584 loss 0.1779944398779644 \n",
            "train acc 0.4569050921339409 loss 0.17688396193185135 \n",
            "train acc 0.46007529225282345 loss 0.17620332819087053 \n",
            "train acc 0.4632454923717059 loss 0.17525821190037125 \n",
            "train acc 0.4662175549831583 loss 0.1746521485548827 \n",
            "train acc 0.4693877551020408 loss 0.17377262212670383 \n",
            "train acc 0.4723598177134931 loss 0.17411276891495392 \n",
            "train acc 0.4753318803249455 loss 0.1747070430553978 \n",
            "train acc 0.4781058054289677 loss 0.17705330199931027 \n",
            "train acc 0.4808797305329899 loss 0.17785745210712775 \n",
            "train acc 0.4838517931444422 loss 0.17957278071491986 \n",
            "train acc 0.48702199326332474 loss 0.17855359141910335 \n",
            "train acc 0.4901921933822072 loss 0.17776280848960752 \n",
            "train acc 0.4931642559936596 loss 0.17845451220211278 \n",
            "train acc 0.49633445611254207 loss 0.17739246737381273 \n",
            "train acc 0.49930651872399445 loss 0.177452759629295 \n",
            "train acc 0.5024767188428769 loss 0.17651504194810094 \n",
            "train acc 0.5056469189617594 loss 0.17565549813887282 \n",
            "train acc 0.508817119080642 loss 0.17464532701508004 \n",
            "train acc 0.5117891816920943 loss 0.17552999216953621 \n",
            "train acc 0.5149593818109768 loss 0.17455987842502999 \n",
            "train acc 0.5179314444224291 loss 0.17461676706105125 \n",
            "train acc 0.5205072320190212 loss 0.1766986738502807 \n",
            "train acc 0.5232811571230434 loss 0.1775829521510964 \n",
            "train acc 0.5264513572419258 loss 0.17659813670175417 \n",
            "train acc 0.5294234198533783 loss 0.17594954259270293 \n",
            "train acc 0.5323954824648306 loss 0.1755474536642853 \n",
            "train acc 0.5355656825837131 loss 0.17471969814113017 \n",
            "train acc 0.5385377451951654 loss 0.17504338354038793 \n",
            "train acc 0.5417079453140479 loss 0.1741416566953477 \n",
            "train acc 0.5448781454329304 loss 0.17341025458206294 \n",
            "train acc 0.5480483455518129 loss 0.17248815371767506 \n",
            "train acc 0.5510204081632653 loss 0.17290662196490283 \n",
            "train acc 0.5535961957598573 loss 0.17453171839208706 \n",
            "train acc 0.5563701208638795 loss 0.1754594876154049 \n",
            "train acc 0.5593421834753318 loss 0.17515720775531185 \n",
            "train acc 0.5625123835942144 loss 0.17437965675869727 \n",
            "train acc 0.5656825837130969 loss 0.17346539712719064 \n",
            "train acc 0.5686546463245492 loss 0.17317629423948427 \n",
            "train acc 0.5716267089360015 loss 0.17484706839987715 \n",
            "train acc 0.574796909054884 loss 0.17395939646015696 \n",
            "train acc 0.5777689716663365 loss 0.1735597606287532 \n",
            "train acc 0.5807410342777888 loss 0.17337534506443328 \n",
            "train acc 0.5837130968892411 loss 0.17358115057403356 \n",
            "train acc 0.5866851595006934 loss 0.17426925564471346 \n",
            "train acc 0.5898553596195759 loss 0.17363971579411275 \n",
            "train acc 0.5926292847235981 loss 0.17438309311838396 \n",
            "train acc 0.5956013473350505 loss 0.17452951461650582 \n",
            "train acc 0.598771547453933 loss 0.17371051081095884 \n",
            "train acc 0.6017436100653853 loss 0.17334498562151565 \n",
            "train acc 0.6049138101842678 loss 0.17251641753903446 \n",
            "train acc 0.6080840103031504 loss 0.1717114169496379 \n",
            "train acc 0.6112542104220329 loss 0.17088373044845212 \n",
            "train acc 0.6140281355260551 loss 0.17162148058231846 \n",
            "train acc 0.6170001981375074 loss 0.17106633985006228 \n",
            "train acc 0.6201703982563899 loss 0.1703579868596879 \n",
            "train acc 0.6231424608678422 loss 0.17049932207210342 \n",
            "train acc 0.6261145234792946 loss 0.1706605167575897 \n",
            "train acc 0.6292847235981771 loss 0.170231929458856 \n",
            "train acc 0.6322567862096294 loss 0.16973956292051645 \n",
            "train acc 0.6354269863285119 loss 0.1691498552047387 \n",
            "train acc 0.6385971864473945 loss 0.16843842493616185 \n",
            "train acc 0.6413711115514167 loss 0.17044294121819484 \n",
            "train acc 0.6445413116702992 loss 0.16972444955886246 \n",
            "train acc 0.6473152367743213 loss 0.17060648456735666 \n",
            "train acc 0.6500891618783435 loss 0.17228946232029962 \n",
            "train acc 0.653259361997226 loss 0.17161689702987945 \n",
            "train acc 0.6564295621161086 loss 0.17086805880855804 \n",
            "train acc 0.6592034872201308 loss 0.17166088214441794 \n",
            "train acc 0.6621755498315831 loss 0.1720174626785923 \n",
            "train acc 0.6653457499504656 loss 0.17143578265216286 \n",
            "train acc 0.6681196750544878 loss 0.17154388751431898 \n",
            "train acc 0.6712898751733702 loss 0.1708237733107244 \n",
            "train acc 0.6738656627699623 loss 0.17194010701496154 \n",
            "train acc 0.6770358628888449 loss 0.1713905494577355 \n",
            "train acc 0.6802060630077273 loss 0.17069753801318266 \n",
            "train acc 0.6833762631266098 loss 0.170044151134789 \n",
            "train acc 0.6865464632454923 loss 0.1694754849457623 \n",
            "train acc 0.6895185258569447 loss 0.16944552207666583 \n",
            "train acc 0.6926887259758272 loss 0.16876191915134373 \n",
            "train acc 0.6958589260947097 loss 0.1680518298962222 \n",
            "train acc 0.698830988706162 loss 0.1682210121954117 \n",
            "train acc 0.7018030513176143 loss 0.16813166559845721 \n",
            "train acc 0.7045769764216365 loss 0.16889358253385395 \n",
            "train acc 0.7071527640182286 loss 0.17150895239270114 \n",
            "train acc 0.7099266891222508 loss 0.17214556971675385 \n",
            "train acc 0.7130968892411333 loss 0.17144114720214645 \n",
            "train acc 0.7160689518525857 loss 0.1709286567222859 \n",
            "train acc 0.7192391519714681 loss 0.17036033359017208 \n",
            "train acc 0.7222112145829205 loss 0.17079466875875368 \n",
            "train acc 0.7253814147018031 loss 0.17011881588390135 \n",
            "train acc 0.7283534773132554 loss 0.16974186293457535 \n",
            "train acc 0.7315236774321379 loss 0.16923630620456404 \n",
            "train acc 0.7344957400435902 loss 0.170040164474917 \n",
            "train acc 0.7376659401624727 loss 0.16942127302517088 \n",
            "train acc 0.7406380027739251 loss 0.16901290299734328 \n",
            "train acc 0.7434119278779473 loss 0.169360568449052 \n",
            "train acc 0.7463839904893996 loss 0.1691599511503873 \n",
            "train acc 0.749356053100852 loss 0.16899468888524904 \n",
            "train acc 0.7523281157123043 loss 0.16867390851303934 \n",
            "train acc 0.7553001783237566 loss 0.16868868735489023 \n",
            "train acc 0.7584703784426392 loss 0.16805820103867777 \n",
            "train acc 0.7616405785615217 loss 0.16744394962981402 \n",
            "train acc 0.7648107786804041 loss 0.16698023741771503 \n",
            "train acc 0.7677828412918565 loss 0.16736668284398085 \n",
            "train acc 0.7707549039033088 loss 0.16709910135614336 \n",
            "train acc 0.7737269665147612 loss 0.1676181357694565 \n",
            "train acc 0.7766990291262136 loss 0.1672392243252302 \n",
            "train acc 0.779869229245096 loss 0.1668792083377601 \n",
            "train acc 0.7830394293639785 loss 0.16632524927576575 \n",
            "train acc 0.7860114919754309 loss 0.1661254817948202 \n",
            "train acc 0.7889835545868833 loss 0.16674219988613526 \n",
            "train acc 0.7921537547057658 loss 0.16614478750742206 \n",
            "train acc 0.7953239548246482 loss 0.16568728240743053 \n",
            "train acc 0.7982960174361006 loss 0.16583004833532955 \n",
            "train acc 0.8014662175549832 loss 0.1652462454967873 \n",
            "train acc 0.8044382801664355 loss 0.1658919638315781 \n",
            "train acc 0.807608480285318 loss 0.16563404189881437 \n",
            "train acc 0.8107786804042004 loss 0.16518262166834566 \n",
            "train acc 0.8137507430156529 loss 0.16520433177205698 \n",
            "train acc 0.8169209431345353 loss 0.164621435236392 \n",
            "train acc 0.8200911432534178 loss 0.16412978439920528 \n",
            "train acc 0.82286506835744 loss 0.16502435004591068 \n",
            "train acc 0.8260352684763225 loss 0.16444654256125818 \n",
            "train acc 0.8288091935803447 loss 0.16572774674912746 \n",
            "train acc 0.8317812561917971 loss 0.16568623849467468 \n",
            "train acc 0.8345551812958193 loss 0.1660973353207192 \n",
            "train acc 0.8377253814147018 loss 0.16551724222167094 \n",
            "train acc 0.8406974440261541 loss 0.16562669535791735 \n",
            "train acc 0.8438676441450366 loss 0.16504874745151027 \n",
            "train acc 0.846839706756489 loss 0.1659402404447812 \n",
            "train acc 0.8498117693679413 loss 0.1660542267173238 \n",
            "train acc 0.8529819694868238 loss 0.1655243440175778 \n",
            "train acc 0.8561521696057063 loss 0.16502634369814112 \n",
            "train acc 0.8589260947097285 loss 0.16595610241127903 \n",
            "train acc 0.862096294828611 loss 0.16542737332654636 \n",
            "train acc 0.8652664949474935 loss 0.16486282915344222 \n",
            "train acc 0.868436695066376 loss 0.16432403134078616 \n",
            "train acc 0.8712106201703982 loss 0.1649554367966718 \n",
            "train acc 0.8743808202892808 loss 0.1644090732061786 \n",
            "train acc 0.8773528829007331 loss 0.16417973902863786 \n",
            "train acc 0.8803249455121854 loss 0.16403753756488074 \n",
            "train acc 0.8832970081236378 loss 0.1638964902269479 \n",
            "train acc 0.8864672082425202 loss 0.16336107117656087 \n",
            "train acc 0.8896374083614028 loss 0.16298711830750107 \n",
            "train acc 0.8928076084802853 loss 0.1624830642579803 \n",
            "train acc 0.8959778085991678 loss 0.16199214121339348 \n",
            "train acc 0.8991480087180502 loss 0.16146097453107944 \n",
            "train acc 0.9021200713295027 loss 0.16128038884962856 \n",
            "train acc 0.9052902714483851 loss 0.1607882159568059 \n",
            "train acc 0.9084604715672676 loss 0.1602941036055737 \n",
            "train acc 0.9112343966712898 loss 0.16107125900337313 \n",
            "train acc 0.9142064592827421 loss 0.1620305302935551 \n",
            "train acc 0.9171785218941946 loss 0.16235005251962176 \n",
            "train acc 0.920348722013077 loss 0.16187754106616264 \n",
            "train acc 0.9235189221319595 loss 0.1613664856124243 \n",
            "train acc 0.926689122250842 loss 0.1609005315179242 \n",
            "train acc 0.9298593223697246 loss 0.16045889388936171 \n",
            "train acc 0.933029522488607 loss 0.15998268943554256 \n",
            "train acc 0.9361997226074895 loss 0.1594837939419273 \n",
            "train acc 0.9393699227263721 loss 0.1589976873127534 \n",
            "train acc 0.9425401228452546 loss 0.1585722065923246 \n",
            "train acc 0.945710322964137 loss 0.15808393871307538 \n",
            "train acc 0.9486823855755894 loss 0.15790927176034542 \n",
            "train acc 0.9516544481870417 loss 0.15772304178570354 \n",
            "train acc 0.9530414107390528 loss 0.15723346614667885 \n",
            "Train loss 0.15723346614667885 accuracy 0.9530414107390528 accuracy1 0.951852585694472\n",
            "val acc 0.022187004754358162 loss 0.13543429970741272 \n",
            "val acc 0.04754358161648178 loss 0.07213837374001741 \n",
            "val acc 0.07131537242472266 loss 0.08302639486889045 \n",
            "val acc 0.09667194928684628 loss 0.06354191014543176 \n",
            "val acc 0.12202852614896989 loss 0.05124363959766924 \n",
            "val acc 0.1473851030110935 loss 0.04557317111175507 \n",
            "val acc 0.17274167987321712 loss 0.03934019325034959 \n",
            "val acc 0.1949286846275753 loss 0.06915918318554759 \n",
            "val acc 0.21870047543581617 loss 0.09025650843977928 \n",
            "val acc 0.24088748019017434 loss 0.12273849211633206 \n",
            "val acc 0.2662440570522979 loss 0.11208446514369412 \n",
            "val acc 0.2916006339144216 loss 0.10634743441672374 \n",
            "val acc 0.31695721077654515 loss 0.09870026414640821 \n",
            "val acc 0.3407290015847861 loss 0.09975561057217419 \n",
            "val acc 0.36450079239302696 loss 0.10690235380704204 \n",
            "val acc 0.38510301109350237 loss 0.14006367776892148 \n",
            "val acc 0.4088748019017433 loss 0.1350016897160779 \n",
            "val acc 0.4342313787638669 loss 0.12822969141416252 \n",
            "val acc 0.4580031695721078 loss 0.1259217992375948 \n",
            "val acc 0.48019017432646594 loss 0.14375130913686007 \n",
            "val acc 0.5039619651347068 loss 0.13938727786409713 \n",
            "val acc 0.5293185419968305 loss 0.13342052048326217 \n",
            "val acc 0.554675118858954 loss 0.1277145280846921 \n",
            "val acc 0.5800316957210777 loss 0.12250054164906032 \n",
            "val acc 0.6038034865293186 loss 0.124661504952237 \n",
            "val acc 0.6291600633914421 loss 0.12066879104643774 \n",
            "val acc 0.6545166402535658 loss 0.11759736591570631 \n",
            "val acc 0.6767036450079239 loss 0.12265326943348295 \n",
            "val acc 0.6988906497622821 loss 0.15003021018868634 \n",
            "val acc 0.7242472266244058 loss 0.1451382036708916 \n",
            "val acc 0.7496038034865293 loss 0.14070732022575552 \n",
            "val acc 0.774960380348653 loss 0.13641209868364967 \n",
            "val acc 0.8003169572107766 loss 0.13246075032899776 \n",
            "val acc 0.8256735340729002 loss 0.1288794845561771 \n",
            "val acc 0.8510301109350238 loss 0.1252874174221818 \n",
            "val acc 0.8763866877971475 loss 0.121878724040774 \n",
            "val acc 0.9001584786053883 loss 0.12220820292478075 \n",
            "val acc 0.9223454833597464 loss 0.13066073380851825 \n",
            "val acc 0.9477020602218701 loss 0.12750972633082897 \n",
            "val acc 0.9587955625990492 loss 0.12671624121721833 \n",
            "Val   loss 0.12671624121721833 accuracy 0.9587955625990492 accuracy1 0.9587955625990491\n",
            "Epoch 3/5\n",
            "----------\n",
            "train acc 0.0029720626114523477 loss 0.19824214279651642 \n",
            "train acc 0.005944125222904695 loss 0.1818588674068451 \n",
            "train acc 0.0091143253417872 loss 0.12410773088534673 \n",
            "train acc 0.012284525460669705 loss 0.09463160065934062 \n",
            "train acc 0.014860313057261739 loss 0.26601789258420466 \n",
            "train acc 0.018030513176144243 loss 0.2223548140997688 \n",
            "train acc 0.02120071329502675 loss 0.1914331255081509 \n",
            "train acc 0.02437091341390925 loss 0.1677368357195519 \n",
            "train acc 0.027541113532791758 loss 0.15108399729554853 \n",
            "train acc 0.030513176144244102 loss 0.16483563012443483 \n",
            "train acc 0.03348523875569645 loss 0.15611903145062653 \n",
            "train acc 0.036655438874578956 loss 0.14326736544414112 \n",
            "train acc 0.03982563899346146 loss 0.13240038687721467 \n",
            "train acc 0.04299583911234396 loss 0.1254110740597493 \n",
            "train acc 0.04616603923122647 loss 0.1174429632568111 \n",
            "train acc 0.04893996433524866 loss 0.14795107555983122 \n",
            "train acc 0.052110164454131164 loss 0.14130920148454607 \n",
            "train acc 0.05528036457301367 loss 0.13385291292797774 \n",
            "train acc 0.058450564691896176 loss 0.12694763617688104 \n",
            "train acc 0.061620764810778675 loss 0.12080142379272729 \n",
            "train acc 0.06479096492966119 loss 0.11526862006368381 \n",
            "train acc 0.06796116504854369 loss 0.11009420800573108 \n",
            "train acc 0.07073509015256588 loss 0.11561844019097803 \n",
            "train acc 0.07370715276401822 loss 0.12338493647015032 \n",
            "train acc 0.07687735288290073 loss 0.11862911167554557 \n",
            "train acc 0.07984941549435308 loss 0.12226429469704342 \n",
            "train acc 0.08282147810580542 loss 0.12226053191935299 \n",
            "train acc 0.08599167822468792 loss 0.1182634354419341 \n",
            "train acc 0.08896374083614028 loss 0.12307092472750308 \n",
            "train acc 0.09213394095502278 loss 0.12033565630360196 \n",
            "train acc 0.09530414107390528 loss 0.11659383762537712 \n",
            "train acc 0.09847434119278779 loss 0.1131674295393168 \n",
            "train acc 0.10144640380424014 loss 0.11855804216765771 \n",
            "train acc 0.10441846641569248 loss 0.11836868915569913 \n",
            "train acc 0.107588666534575 loss 0.11512778612252857 \n",
            "train acc 0.1107588666534575 loss 0.11200337201201667 \n",
            "train acc 0.11392906677233999 loss 0.10906551452353597 \n",
            "train acc 0.1170992668912225 loss 0.1068402789165511 \n",
            "train acc 0.120269467010105 loss 0.10414996409478287 \n",
            "train acc 0.1234396671289875 loss 0.10160410819225944 \n",
            "train acc 0.12641172974043985 loss 0.10153711846711613 \n",
            "train acc 0.12958192985932238 loss 0.09930113093772282 \n",
            "train acc 0.13275212997820487 loss 0.0970192132409402 \n",
            "train acc 0.13592233009708737 loss 0.09484053696823222 \n",
            "train acc 0.13909253021596987 loss 0.0929004334172027 \n",
            "train acc 0.14206459282742223 loss 0.0942745587191261 \n",
            "train acc 0.14503665543887456 loss 0.0954355907656173 \n",
            "train acc 0.1482068555577571 loss 0.09396364099908776 \n",
            "train acc 0.1513770556766396 loss 0.09212577677978089 \n",
            "train acc 0.1545472557955221 loss 0.0907497151242569 \n",
            "train acc 0.1577174559144046 loss 0.08899756852656092 \n",
            "train acc 0.16088765603328709 loss 0.08732299543156002 \n",
            "train acc 0.16405785615216958 loss 0.08577791360461698 \n",
            "train acc 0.1672280562710521 loss 0.08431718157630207 \n",
            "train acc 0.1703982563899346 loss 0.08281957974064756 \n",
            "train acc 0.17297404398652663 loss 0.09279414991033264 \n",
            "train acc 0.175946106597979 loss 0.09970026717749997 \n",
            "train acc 0.17891816920943135 loss 0.10083730267922811 \n",
            "train acc 0.18208836932831385 loss 0.09918449761077636 \n",
            "train acc 0.18525856944719635 loss 0.09757960579590871 \n",
            "train acc 0.18842876956607885 loss 0.09600432258534444 \n",
            "train acc 0.19159896968496135 loss 0.09450677140322965 \n",
            "train acc 0.1945710322964137 loss 0.09700322962210824 \n",
            "train acc 0.1977412324152962 loss 0.09591936155084113 \n",
            "train acc 0.2009114325341787 loss 0.09457179367578086 \n",
            "train acc 0.2040816326530612 loss 0.09317071970629816 \n",
            "train acc 0.20705369526451356 loss 0.09374983328457143 \n",
            "train acc 0.21022389538339606 loss 0.09245166479548275 \n",
            "train acc 0.21339409550227856 loss 0.09115027090149891 \n",
            "train acc 0.21656429562116108 loss 0.09002913351536596 \n",
            "train acc 0.21973449574004358 loss 0.08879302161201362 \n",
            "train acc 0.22270655835149591 loss 0.09180781816798521 \n",
            "train acc 0.22567862096294827 loss 0.09269359842906004 \n",
            "train acc 0.22884882108183077 loss 0.09147351310506614 \n",
            "train acc 0.23201902120071327 loss 0.090347797657984 \n",
            "train acc 0.2351892213195958 loss 0.08920315543869756 \n",
            "train acc 0.2383594214384783 loss 0.08809183638500558 \n",
            "train acc 0.2411333465425005 loss 0.09282027035862064 \n",
            "train acc 0.244303546661383 loss 0.0917279785926324 \n",
            "train acc 0.24707747176540518 loss 0.0960273710617912 \n",
            "train acc 0.2502476718842877 loss 0.09487757797462198 \n",
            "train acc 0.25321973449574003 loss 0.095651813875032 \n",
            "train acc 0.25619179710719236 loss 0.09570850335560874 \n",
            "train acc 0.25916385971864475 loss 0.09679088114145477 \n",
            "train acc 0.26173964731523675 loss 0.10106699968660798 \n",
            "train acc 0.26471170992668913 loss 0.10257904242513008 \n",
            "train acc 0.2678819100455716 loss 0.10141731866089435 \n",
            "train acc 0.27105211016445413 loss 0.10027721070351121 \n",
            "train acc 0.2742223102833366 loss 0.09917901838695359 \n",
            "train acc 0.27739251040221913 loss 0.09811922771364658 \n",
            "train acc 0.28056271052110165 loss 0.09705674088979652 \n",
            "train acc 0.2837329106399841 loss 0.09602253723120235 \n",
            "train acc 0.28690311075886665 loss 0.09504567443703611 \n",
            "train acc 0.2900733108777491 loss 0.09405237763248225 \n",
            "train acc 0.29324351099663165 loss 0.09317647549078653 \n",
            "train acc 0.2964137111155142 loss 0.09222705472105493 \n",
            "train acc 0.29958391123439665 loss 0.09135176916366693 \n",
            "train acc 0.302555973845849 loss 0.09168989757760143 \n",
            "train acc 0.3057261739647315 loss 0.09080586023628712 \n",
            "train acc 0.30889637408361403 loss 0.0901151837222278 \n",
            "train acc 0.3120665742024965 loss 0.08940141266304077 \n",
            "train acc 0.3148404993065187 loss 0.09362783613523432 \n",
            "train acc 0.3176144244105409 loss 0.09652300650995334 \n",
            "train acc 0.32058648702199327 loss 0.09622215639011791 \n",
            "train acc 0.3235585496334456 loss 0.09682156984649953 \n",
            "train acc 0.3267287497523281 loss 0.0959493790638967 \n",
            "train acc 0.32950267485635026 loss 0.09896786163221592 \n",
            "train acc 0.3326728749752328 loss 0.09807859789752574 \n",
            "train acc 0.3358430750941153 loss 0.09728845449830961 \n",
            "train acc 0.33881513770556765 loss 0.0975620578805154 \n",
            "train acc 0.3419853378244502 loss 0.09699441185472785 \n",
            "train acc 0.34515553794333265 loss 0.09620915168696749 \n",
            "train acc 0.3483257380622152 loss 0.09546168560607243 \n",
            "train acc 0.3512978006736675 loss 0.09774704019359329 \n",
            "train acc 0.35446800079255003 loss 0.09691522855554587 \n",
            "train acc 0.35744006340400236 loss 0.09812256920649189 \n",
            "train acc 0.3606102635228849 loss 0.09749251558310074 \n",
            "train acc 0.3635823261343372 loss 0.10014539709614621 \n",
            "train acc 0.3667525262532197 loss 0.09936599740602013 \n",
            "train acc 0.3697245888646721 loss 0.0999154414399527 \n",
            "train acc 0.37289478898355455 loss 0.09941502700921548 \n",
            "train acc 0.3760649891024371 loss 0.09861549593844131 \n",
            "train acc 0.3790370517138894 loss 0.10132420649827738 \n",
            "train acc 0.3820091143253418 loss 0.10268663812518841 \n",
            "train acc 0.384783039429364 loss 0.10679634790867568 \n",
            "train acc 0.38795323954824645 loss 0.10596161045130371 \n",
            "train acc 0.391123439667129 loss 0.1051473101045511 \n",
            "train acc 0.39429363978601145 loss 0.10435409486763092 \n",
            "train acc 0.39726570239746384 loss 0.10562658810341918 \n",
            "train acc 0.4004359025163463 loss 0.10484782914147497 \n",
            "train acc 0.40360610263522884 loss 0.10408482835041573 \n",
            "train acc 0.40657816524668117 loss 0.10516695952780235 \n",
            "train acc 0.4097483653655637 loss 0.10441303963307291 \n",
            "train acc 0.41291856548444617 loss 0.10390343259995692 \n",
            "train acc 0.41589062809589855 loss 0.10570396448537293 \n",
            "train acc 0.4188626907073509 loss 0.10690370989665526 \n",
            "train acc 0.4220328908262334 loss 0.10616445784834315 \n",
            "train acc 0.4246086784228254 loss 0.11124513270012845 \n",
            "train acc 0.42777887854170793 loss 0.11054740795989608 \n",
            "train acc 0.43075094115316026 loss 0.11123388232081197 \n",
            "train acc 0.4339211412720428 loss 0.11053704312616126 \n",
            "train acc 0.43709134139092526 loss 0.10980853446001763 \n",
            "train acc 0.44006340400237764 loss 0.11085390093511661 \n",
            "train acc 0.4432336041212601 loss 0.11015792340185726 \n",
            "train acc 0.44640380424014264 loss 0.10949387688848093 \n",
            "train acc 0.449375866851595 loss 0.10982137443641303 \n",
            "train acc 0.4525460669704775 loss 0.1091116411796752 \n",
            "train acc 0.45571626708935997 loss 0.10842120785319649 \n",
            "train acc 0.45868832970081236 loss 0.11005266637763246 \n",
            "train acc 0.4616603923122647 loss 0.111823281814189 \n",
            "train acc 0.4648305924311472 loss 0.11120143755484027 \n",
            "train acc 0.4680007925500297 loss 0.11054772465247784 \n",
            "train acc 0.4711709926689122 loss 0.11001130392595371 \n",
            "train acc 0.4743411927877947 loss 0.10931389581173198 \n",
            "train acc 0.4775113929066772 loss 0.10863601824103464 \n",
            "train acc 0.48068159302555974 loss 0.10804021195284665 \n",
            "train acc 0.48345551812958193 loss 0.10902188242944016 \n",
            "train acc 0.48642758074103426 loss 0.10963632476801949 \n",
            "train acc 0.4893996433524866 loss 0.11110032025692435 \n",
            "train acc 0.4925698434713691 loss 0.11046534486013115 \n",
            "train acc 0.49554190608282145 loss 0.11045992960184466 \n",
            "train acc 0.498712106201704 loss 0.10979205375626384 \n",
            "train acc 0.5018823063205865 loss 0.10916216034610739 \n",
            "train acc 0.505052506439469 loss 0.10866029377353786 \n",
            "train acc 0.5082227065583514 loss 0.108014538491173 \n",
            "train acc 0.511392906677234 loss 0.10775870992731953 \n",
            "train acc 0.5145631067961165 loss 0.10723026036800716 \n",
            "train acc 0.517733306914999 loss 0.10664222896905683 \n",
            "train acc 0.5209035070338814 loss 0.10602431425193777 \n",
            "train acc 0.5238755696453339 loss 0.10581911384223905 \n",
            "train acc 0.5270457697642164 loss 0.10521407692341886 \n",
            "train acc 0.5300178323756687 loss 0.1050142428064773 \n",
            "train acc 0.5331880324945512 loss 0.10447958679775562 \n",
            "train acc 0.5361600951060035 loss 0.10459188320045092 \n",
            "train acc 0.5393302952248861 loss 0.10400708503954645 \n",
            "train acc 0.5425004953437685 loss 0.10348623306212672 \n",
            "train acc 0.545670695462651 loss 0.10295019182442335 \n",
            "train acc 0.5486427580741035 loss 0.10286628586185176 \n",
            "train acc 0.5518129581929859 loss 0.10231278382967533 \n",
            "train acc 0.5549831583118684 loss 0.10177857926205938 \n",
            "train acc 0.5581533584307509 loss 0.10127915441153766 \n",
            "train acc 0.5613235585496335 loss 0.10074109819162494 \n",
            "train acc 0.5644937586685159 loss 0.10053316073917117 \n",
            "train acc 0.5672676837725381 loss 0.10469059554705619 \n",
            "train acc 0.5702397463839904 loss 0.10524457908248076 \n",
            "train acc 0.5732118089954428 loss 0.10508216590738244 \n",
            "train acc 0.5763820091143254 loss 0.10453780169441877 \n",
            "train acc 0.5795522092332078 loss 0.10398801081650395 \n",
            "train acc 0.5827224093520903 loss 0.10344869953028306 \n",
            "train acc 0.5856944719635426 loss 0.10491465093004272 \n",
            "train acc 0.5888646720824252 loss 0.10437417647274728 \n",
            "train acc 0.5920348722013077 loss 0.10385416715022681 \n",
            "train acc 0.5952050723201902 loss 0.10352255432104084 \n",
            "train acc 0.5981771349316425 loss 0.1034572855437405 \n",
            "train acc 0.6011491975430949 loss 0.10398497479394651 \n",
            "train acc 0.6043193976619774 loss 0.10347777211264118 \n",
            "train acc 0.6072914602734297 loss 0.10470141812350582 \n",
            "train acc 0.6102635228848821 loss 0.10443002468732308 \n",
            "train acc 0.6134337230037645 loss 0.10392208058789522 \n",
            "train acc 0.6166039231226471 loss 0.10347144189523533 \n",
            "train acc 0.6197741232415296 loss 0.10296876360639114 \n",
            "train acc 0.6229443233604121 loss 0.10246852597989382 \n",
            "train acc 0.6261145234792946 loss 0.10197383505831303 \n",
            "train acc 0.6288884485833168 loss 0.10294705690580931 \n",
            "train acc 0.6320586487021993 loss 0.10246829016690664 \n",
            "train acc 0.6352288488210818 loss 0.1019918527753653 \n",
            "train acc 0.6382009114325341 loss 0.10284185479113896 \n",
            "train acc 0.6413711115514167 loss 0.10251171318556701 \n",
            "train acc 0.6445413116702992 loss 0.10215125642282416 \n",
            "train acc 0.6477115117891816 loss 0.10170600128060739 \n",
            "train acc 0.6508817119080642 loss 0.10130421702818442 \n",
            "train acc 0.6540519120269467 loss 0.100847042768091 \n",
            "train acc 0.6568258371309689 loss 0.10299032544028539 \n",
            "train acc 0.6599960372498513 loss 0.10253126726313885 \n",
            "train acc 0.6629680998613037 loss 0.10280621816176748 \n",
            "train acc 0.6657420249653259 loss 0.10461404231749906 \n",
            "train acc 0.6689122250842084 loss 0.1041578918331719 \n",
            "train acc 0.6720824252030909 loss 0.10368883556156598 \n",
            "train acc 0.6748563503071131 loss 0.10462377878059059 \n",
            "train acc 0.6780265504259956 loss 0.10416264874035154 \n",
            "train acc 0.6811967505448782 loss 0.10372672748027108 \n",
            "train acc 0.6843669506637606 loss 0.10340320915172531 \n",
            "train acc 0.6875371507826431 loss 0.10294725628979003 \n",
            "train acc 0.6905092133940954 loss 0.10437044747439879 \n",
            "train acc 0.693679413512978 loss 0.10399050720863873 \n",
            "train acc 0.6968496136318605 loss 0.10355036985747253 \n",
            "train acc 0.700019813750743 loss 0.10310241977821642 \n",
            "train acc 0.7031900138696254 loss 0.10279751097499557 \n",
            "train acc 0.7061620764810779 loss 0.10324781182106162 \n",
            "train acc 0.7093322765999603 loss 0.10281710186821368 \n",
            "train acc 0.7125024767188428 loss 0.10237790371323593 \n",
            "train acc 0.7156726768377254 loss 0.10197831612935386 \n",
            "train acc 0.7186447394491777 loss 0.10203861336167822 \n",
            "train acc 0.7214186645531999 loss 0.10289467683034097 \n",
            "train acc 0.7241925896572221 loss 0.10382993490474814 \n",
            "train acc 0.7273627897761046 loss 0.10341770944118468 \n",
            "train acc 0.730532989894987 loss 0.10299547810406122 \n",
            "train acc 0.7337031900138696 loss 0.10257228652011416 \n",
            "train acc 0.7368733901327521 loss 0.10216427407866523 \n",
            "train acc 0.7398454527442044 loss 0.10248997270925125 \n",
            "train acc 0.7430156528630869 loss 0.1021048470846613 \n",
            "train acc 0.7459877154745392 loss 0.10211764657889177 \n",
            "train acc 0.7491579155934218 loss 0.1017500906119146 \n",
            "train acc 0.7521299782048742 loss 0.10244172355102864 \n",
            "train acc 0.7553001783237566 loss 0.10203205406741829 \n",
            "train acc 0.7584703784426392 loss 0.10163349128419125 \n",
            "train acc 0.7616405785615217 loss 0.10126677708066849 \n",
            "train acc 0.7648107786804041 loss 0.1009795096864353 \n",
            "train acc 0.7679809787992866 loss 0.1006668531896928 \n",
            "train acc 0.7711511789181692 loss 0.10039479961059987 \n",
            "train acc 0.7743213790370517 loss 0.10004514229272407 \n",
            "train acc 0.7774915791559341 loss 0.09965877795392382 \n",
            "train acc 0.7806617792748167 loss 0.09927875024502132 \n",
            "train acc 0.7838319793936992 loss 0.09891939341497615 \n",
            "train acc 0.7866059044977214 loss 0.09950648858211934 \n",
            "train acc 0.7897761046166039 loss 0.0991310353638255 \n",
            "train acc 0.7929463047354864 loss 0.09878352415613859 \n",
            "train acc 0.7961165048543689 loss 0.09840777439567036 \n",
            "train acc 0.7992867049732514 loss 0.09804476994363066 \n",
            "train acc 0.8024569050921339 loss 0.0977047865646175 \n",
            "train acc 0.8056271052110164 loss 0.09740369251779356 \n",
            "train acc 0.8085991678224688 loss 0.09849959425717052 \n",
            "train acc 0.8117693679413512 loss 0.09813602646890703 \n",
            "train acc 0.8149395680602337 loss 0.0977801220512695 \n",
            "train acc 0.8181097681791163 loss 0.09742415369101712 \n",
            "train acc 0.8212799682979988 loss 0.09706622409490742 \n",
            "train acc 0.8242520309094511 loss 0.09825613818215101 \n",
            "train acc 0.8274222310283336 loss 0.09792399462468838 \n",
            "train acc 0.8305924311472161 loss 0.09758634704206706 \n",
            "train acc 0.8335644937586685 loss 0.09814266324715895 \n",
            "train acc 0.836734693877551 loss 0.09778747340489 \n",
            "train acc 0.8399048939964335 loss 0.09743170996336951 \n",
            "train acc 0.8428769566078859 loss 0.09738711658063526 \n",
            "train acc 0.8460471567267683 loss 0.09703896312131445 \n",
            "train acc 0.8488210818307905 loss 0.09825037092478438 \n",
            "train acc 0.851991281949673 loss 0.09798970317596273 \n",
            "train acc 0.8547652070536952 loss 0.0989070078105891 \n",
            "train acc 0.8579354071725778 loss 0.09855932797633594 \n",
            "train acc 0.8611056072914602 loss 0.09821533264198397 \n",
            "train acc 0.8642758074103427 loss 0.09786953164189721 \n",
            "train acc 0.8674460075292252 loss 0.09753251876690329 \n",
            "train acc 0.8706162076481078 loss 0.09720869328874553 \n",
            "train acc 0.8737864077669902 loss 0.09687068354126145 \n",
            "train acc 0.8769566078858727 loss 0.09654318597077549 \n",
            "train acc 0.8797305329898949 loss 0.09741833683863087 \n",
            "train acc 0.8829007331087775 loss 0.0970843234714088 \n",
            "train acc 0.88607093322766 loss 0.09675283165517155 \n",
            "train acc 0.8892411333465424 loss 0.09642586793254143 \n",
            "train acc 0.8920150584505646 loss 0.09762119007688402 \n",
            "train acc 0.8951852585694472 loss 0.097289387451822 \n",
            "train acc 0.8983554586883297 loss 0.09697369196655427 \n",
            "train acc 0.9015256588072121 loss 0.09668021770521093 \n",
            "train acc 0.9046958589260946 loss 0.09639972856041018 \n",
            "train acc 0.9078660590449772 loss 0.09608217290734086 \n",
            "train acc 0.9110362591638597 loss 0.09579443823254147 \n",
            "train acc 0.9142064592827421 loss 0.09547672369454459 \n",
            "train acc 0.9173766594016247 loss 0.09516303054518019 \n",
            "train acc 0.9205468595205072 loss 0.09485170951475633 \n",
            "train acc 0.9237170596393897 loss 0.09454813514396947 \n",
            "train acc 0.9268872597582722 loss 0.09424452126569426 \n",
            "train acc 0.9300574598771547 loss 0.09394002126443178 \n",
            "train acc 0.933029522488607 loss 0.09442191104564988 \n",
            "train acc 0.9360015851000594 loss 0.09506196405572614 \n",
            "train acc 0.9389736477115117 loss 0.09534074907577106 \n",
            "train acc 0.9421438478303943 loss 0.09504802873602412 \n",
            "train acc 0.9453140479492768 loss 0.09474939385085104 \n",
            "train acc 0.9484842480681592 loss 0.0944491675941134 \n",
            "train acc 0.9516544481870417 loss 0.09420046861232077 \n",
            "train acc 0.9548246483059243 loss 0.09390490573776651 \n",
            "train acc 0.9579948484248068 loss 0.09360822052206663 \n",
            "train acc 0.9611650485436892 loss 0.09331177576209966 \n",
            "train acc 0.9643352486625718 loss 0.09302327689343096 \n",
            "train acc 0.9675054487814543 loss 0.09273241790861023 \n",
            "train acc 0.9706756489003368 loss 0.09247912382847885 \n",
            "train acc 0.9738458490192193 loss 0.09225348393794977 \n",
            "train acc 0.9752328115712304 loss 0.09196440868014824 \n",
            "Train loss 0.09196440868014824 accuracy 0.9752328115712304 accuracy1 0.9752328115712304\n",
            "val acc 0.025356576862123614 loss 0.029711000621318817 \n",
            "val acc 0.05071315372424723 loss 0.015849198447540402 \n",
            "val acc 0.07448494453248812 loss 0.032840554447223745 \n",
            "val acc 0.09984152139461173 loss 0.024968209210783243 \n",
            "val acc 0.12519809825673534 loss 0.020195872499607505 \n",
            "val acc 0.15055467511885895 loss 0.017687806684989482 \n",
            "val acc 0.17591125198098256 loss 0.015286921972541936 \n",
            "val acc 0.19809825673534073 loss 0.0488888207328273 \n",
            "val acc 0.22187004754358164 loss 0.0980166325526726 \n",
            "val acc 0.24405705229793978 loss 0.1370819028816186 \n",
            "val acc 0.2694136291600634 loss 0.1248192345533012 \n",
            "val acc 0.294770206022187 loss 0.11525668711207497 \n",
            "val acc 0.3201267828843106 loss 0.10658712693167707 \n",
            "val acc 0.3438985736925515 loss 0.11957444364504356 \n",
            "val acc 0.3660855784469097 loss 0.11892850058308492 \n",
            "val acc 0.3898573692551506 loss 0.13990773686236935 \n",
            "val acc 0.4152139461172742 loss 0.13206194141907068 \n",
            "val acc 0.4405705229793978 loss 0.12578276853309944 \n",
            "val acc 0.4643423137876387 loss 0.1333256071543713 \n",
            "val acc 0.4865293185419968 loss 0.15776795948040673 \n",
            "val acc 0.5103011093502378 loss 0.1652568484534554 \n",
            "val acc 0.5356576862123613 loss 0.15777559697479857 \n",
            "val acc 0.561014263074485 loss 0.15095472116650932 \n",
            "val acc 0.5863708399366085 loss 0.14471225767435195 \n",
            "val acc 0.6069730586370841 loss 0.15613456324674188 \n",
            "val acc 0.6323296354992076 loss 0.1504041001899168 \n",
            "val acc 0.6561014263074485 loss 0.1518233953371506 \n",
            "val acc 0.6798732171156894 loss 0.1620506372419186 \n",
            "val acc 0.7020602218700476 loss 0.19122592998058374 \n",
            "val acc 0.7274167987321711 loss 0.18498424189941337 \n",
            "val acc 0.7527733755942948 loss 0.1795591288771961 \n",
            "val acc 0.7781299524564184 loss 0.17398287925607292 \n",
            "val acc 0.803486529318542 loss 0.16894806861510556 \n",
            "val acc 0.8288431061806656 loss 0.164102891987354 \n",
            "val acc 0.8541996830427893 loss 0.15944545367133936 \n",
            "val acc 0.8795562599049128 loss 0.15504218203326067 \n",
            "val acc 0.9033280507131538 loss 0.15311507055082837 \n",
            "val acc 0.9270998415213947 loss 0.15584621088285194 \n",
            "val acc 0.9524564183835182 loss 0.1528304752248984 \n",
            "val acc 0.9619651347068147 loss 0.1534341225400567 \n",
            "Val   loss 0.1534341225400567 accuracy 0.9619651347068147 accuracy1 0.9619651347068146\n",
            "Epoch 4/5\n",
            "----------\n",
            "train acc 0.0031702001188825043 loss 0.008161616511642933 \n",
            "train acc 0.006340400237765009 loss 0.006384578999131918 \n",
            "train acc 0.009510600356647513 loss 0.00507076526992023 \n",
            "train acc 0.012680800475530017 loss 0.004114485462196171 \n",
            "train acc 0.015454725579552209 loss 0.07518128687515854 \n",
            "train acc 0.018624925698434713 loss 0.06291863196141396 \n",
            "train acc 0.021795125817317216 loss 0.05415674155977156 \n",
            "train acc 0.024965325936199722 loss 0.047494234604528174 \n",
            "train acc 0.028135526055082224 loss 0.044022145862173706 \n",
            "train acc 0.03130572617396473 loss 0.04354089319240302 \n",
            "train acc 0.03447592629284724 loss 0.04175337969156152 \n",
            "train acc 0.037646126411729736 loss 0.03834877051122021 \n",
            "train acc 0.04081632653061224 loss 0.035476732887148574 \n",
            "train acc 0.04398652664949475 loss 0.03461873973304007 \n",
            "train acc 0.047156726768377254 loss 0.03238846609601751 \n",
            "train acc 0.049930651872399444 loss 0.07161267241099267 \n",
            "train acc 0.05310085199128195 loss 0.06754575884449021 \n",
            "train acc 0.05627105211016445 loss 0.06418843658886747 \n",
            "train acc 0.059441252229046955 loss 0.06087244631367197 \n",
            "train acc 0.06261145234792946 loss 0.059234952516271734 \n",
            "train acc 0.06578165246681196 loss 0.05646087429652523 \n",
            "train acc 0.06895185258569447 loss 0.05393699327727187 \n",
            "train acc 0.07212205270457697 loss 0.05169628978372835 \n",
            "train acc 0.07509411531602932 loss 0.05969802131827843 \n",
            "train acc 0.07826431543491183 loss 0.05743384723318741 \n",
            "train acc 0.08143451555379433 loss 0.055341477145422965 \n",
            "train acc 0.08460471567267683 loss 0.053366565755654675 \n",
            "train acc 0.08777491579155934 loss 0.0515878812937964 \n",
            "train acc 0.09074697840301169 loss 0.057943111901765626 \n",
            "train acc 0.09391717852189418 loss 0.05608749059611 \n",
            "train acc 0.0970873786407767 loss 0.05435791796246063 \n",
            "train acc 0.1002575787596592 loss 0.05269371705617232 \n",
            "train acc 0.10322964137111154 loss 0.05906188114595628 \n",
            "train acc 0.10639984148999405 loss 0.057406651819987664 \n",
            "train acc 0.10957004160887655 loss 0.05583560940244102 \n",
            "train acc 0.11274024172775905 loss 0.054322169506728336 \n",
            "train acc 0.11591044184664157 loss 0.05289145978324971 \n",
            "train acc 0.11908064196552406 loss 0.0517569530255921 \n",
            "train acc 0.12225084208440658 loss 0.050459194234393254 \n",
            "train acc 0.12542104220328906 loss 0.04922730404796312 \n",
            "train acc 0.1285912423221716 loss 0.048215766109401224 \n",
            "train acc 0.1317614424410541 loss 0.04713172892003231 \n",
            "train acc 0.1349316425599366 loss 0.046084131824859785 \n",
            "train acc 0.1381018426788191 loss 0.04505903369202067 \n",
            "train acc 0.14127204279770159 loss 0.04409102582398595 \n",
            "train acc 0.14424410540915394 loss 0.04634408183198463 \n",
            "train acc 0.14701803051317613 loss 0.04751354788970004 \n",
            "train acc 0.15018823063205863 loss 0.04729886787754367 \n",
            "train acc 0.15335843075094113 loss 0.046401732782975824 \n",
            "train acc 0.15652863086982366 loss 0.04551055775838904 \n",
            "train acc 0.15969883098870616 loss 0.04463463442687712 \n",
            "train acc 0.16286903110758866 loss 0.04379914504981851 \n",
            "train acc 0.16603923122647116 loss 0.04300500995387271 \n",
            "train acc 0.16920943134535366 loss 0.04229351688344549 \n",
            "train acc 0.17237963146423618 loss 0.04154477975098416 \n",
            "train acc 0.1753516940756885 loss 0.04424535046252588 \n",
            "train acc 0.17832375668714087 loss 0.04472198705454439 \n",
            "train acc 0.18149395680602337 loss 0.04398446681945392 \n",
            "train acc 0.18466415692490587 loss 0.04326181998969671 \n",
            "train acc 0.18783435704378837 loss 0.04257312454962327 \n",
            "train acc 0.1910045571626709 loss 0.04188700855354641 \n",
            "train acc 0.1941747572815534 loss 0.04123728144294282 \n",
            "train acc 0.1973449574004359 loss 0.040651847219412465 \n",
            "train acc 0.2005151575193184 loss 0.040043289892309986 \n",
            "train acc 0.2036853576382009 loss 0.03948749699540293 \n",
            "train acc 0.2068555577570834 loss 0.03891174225436495 \n",
            "train acc 0.21002575787596592 loss 0.038703741656039466 \n",
            "train acc 0.21319595799484842 loss 0.038166231911471935 \n",
            "train acc 0.21636615811373092 loss 0.037634188723514206 \n",
            "train acc 0.21953635823261342 loss 0.037119544497857405 \n",
            "train acc 0.22270655835149591 loss 0.03660971533038168 \n",
            "train acc 0.22587675847037844 loss 0.036292735792004455 \n",
            "train acc 0.22904695858926094 loss 0.03596393166393782 \n",
            "train acc 0.23221715870814344 loss 0.03549311869082394 \n",
            "train acc 0.23538735882702594 loss 0.03503784389933571 \n",
            "train acc 0.23855755894590844 loss 0.034589934469917545 \n",
            "train acc 0.24172775906479096 loss 0.034183349010576916 \n",
            "train acc 0.2446998216762433 loss 0.03919595220642618 \n",
            "train acc 0.2478700217951258 loss 0.03872581861421018 \n",
            "train acc 0.250643946899148 loss 0.04251122099885833 \n",
            "train acc 0.2538141470180305 loss 0.04201805230623893 \n",
            "train acc 0.256984347136913 loss 0.041645521662203686 \n",
            "train acc 0.2601545472557955 loss 0.041270933114693514 \n",
            "train acc 0.26332474737467804 loss 0.04091643209505405 \n",
            "train acc 0.26629680998613037 loss 0.04134471429689952 \n",
            "train acc 0.2692688725975827 loss 0.04189916357985524 \n",
            "train acc 0.2724390727164652 loss 0.04145600006918305 \n",
            "train acc 0.2756092728353477 loss 0.04099235502905107 \n",
            "train acc 0.2787794729542302 loss 0.04054424045882135 \n",
            "train acc 0.28194967307311275 loss 0.04035567097534012 \n",
            "train acc 0.2851198731919952 loss 0.04004745927938659 \n",
            "train acc 0.28829007331087775 loss 0.03963013598885205 \n",
            "train acc 0.2914602734297602 loss 0.03921445491673645 \n",
            "train acc 0.29463047354864275 loss 0.0388053350851554 \n",
            "train acc 0.2978006736675253 loss 0.038406715730793385 \n",
            "train acc 0.30097087378640774 loss 0.038021803854159465 \n",
            "train acc 0.30414107390529027 loss 0.03764528989763066 \n",
            "train acc 0.30731127402417274 loss 0.03727612735903157 \n",
            "train acc 0.31048147414305527 loss 0.036932197364160056 \n",
            "train acc 0.31365167426193774 loss 0.0365863985195756 \n",
            "train acc 0.31682187438082027 loss 0.036233381526129746 \n",
            "train acc 0.31959579948484246 loss 0.04195097970945176 \n",
            "train acc 0.3225678620962948 loss 0.04344548666691498 \n",
            "train acc 0.3257380622151773 loss 0.04331022535132853 \n",
            "train acc 0.32890826233405984 loss 0.042925658357507064 \n",
            "train acc 0.3320784624529423 loss 0.04276434308959101 \n",
            "train acc 0.3350505250643947 loss 0.044113079152313696 \n",
            "train acc 0.33822072518327717 loss 0.043724393559386954 \n",
            "train acc 0.3413909253021597 loss 0.043349948341086714 \n",
            "train acc 0.34456112542104217 loss 0.042976600116923114 \n",
            "train acc 0.3477313255399247 loss 0.04260824513852361 \n",
            "train acc 0.35090152565880717 loss 0.042254166612110566 \n",
            "train acc 0.3540717257776897 loss 0.04192743601331042 \n",
            "train acc 0.357043788389142 loss 0.04514786260500714 \n",
            "train acc 0.36021398850802455 loss 0.04476607572894705 \n",
            "train acc 0.3633841886269071 loss 0.044708438584950334 \n",
            "train acc 0.36655438874578955 loss 0.044338963497987285 \n",
            "train acc 0.3695264513572419 loss 0.046372623279517106 \n",
            "train acc 0.3726966514761244 loss 0.045999537329698185 \n",
            "train acc 0.37566871408757674 loss 0.04630248654284515 \n",
            "train acc 0.37883891420645927 loss 0.04594491478363591 \n",
            "train acc 0.3820091143253418 loss 0.045578227874624434 \n",
            "train acc 0.3849811769367941 loss 0.046075817371711014 \n",
            "train acc 0.38795323954824645 loss 0.046398729829478166 \n",
            "train acc 0.3909253021596988 loss 0.04935066320747137 \n",
            "train acc 0.3940955022785813 loss 0.0489644639324286 \n",
            "train acc 0.39726570239746384 loss 0.04859023424384265 \n",
            "train acc 0.4004359025163463 loss 0.04821742012836694 \n",
            "train acc 0.4034079651277987 loss 0.051042465403289786 \n",
            "train acc 0.40657816524668117 loss 0.05066325554955536 \n",
            "train acc 0.4097483653655637 loss 0.05029623557943211 \n",
            "train acc 0.412720427977016 loss 0.051031849098790466 \n",
            "train acc 0.41589062809589855 loss 0.05065735662399784 \n",
            "train acc 0.419060828214781 loss 0.050288376022641086 \n",
            "train acc 0.42223102833366355 loss 0.049931593432039435 \n",
            "train acc 0.425401228452546 loss 0.04958019092608832 \n",
            "train acc 0.42857142857142855 loss 0.04933327809613484 \n",
            "train acc 0.4311472161680206 loss 0.05450708865132385 \n",
            "train acc 0.43431741628690307 loss 0.05412994938183965 \n",
            "train acc 0.4374876164057856 loss 0.05377889601804782 \n",
            "train acc 0.4406578165246681 loss 0.05341248500370586 \n",
            "train acc 0.4438280166435506 loss 0.05304889591996112 \n",
            "train acc 0.4468000792550029 loss 0.05497713398613309 \n",
            "train acc 0.44997027937388545 loss 0.054722995573683875 \n",
            "train acc 0.453140479492768 loss 0.05435804457861352 \n",
            "train acc 0.45631067961165045 loss 0.05399373354277038 \n",
            "train acc 0.459480879730533 loss 0.053642147957572245 \n",
            "train acc 0.46265107984941545 loss 0.053290143725855554 \n",
            "train acc 0.46542500495343764 loss 0.05635179260834714 \n",
            "train acc 0.46839706756489 loss 0.05677425331474903 \n",
            "train acc 0.4715672676837725 loss 0.05641765319564941 \n",
            "train acc 0.474737467802655 loss 0.05605535635818102 \n",
            "train acc 0.47790766792153755 loss 0.055712050127148555 \n",
            "train acc 0.48107786804042 loss 0.05536840660474868 \n",
            "train acc 0.48424806815930255 loss 0.055020462589022975 \n",
            "train acc 0.487418268278185 loss 0.05473031757290487 \n",
            "train acc 0.49058846839706755 loss 0.054481206597852265 \n",
            "train acc 0.4935605310085199 loss 0.054712163950125035 \n",
            "train acc 0.4965325936199722 loss 0.05534835326677165 \n",
            "train acc 0.49970279373885473 loss 0.05502386091066001 \n",
            "train acc 0.5028729938577372 loss 0.05471024177469579 \n",
            "train acc 0.5060431939766198 loss 0.05437953631288694 \n",
            "train acc 0.5092133940955023 loss 0.054056648975966695 \n",
            "train acc 0.5123835942143847 loss 0.05373937666959913 \n",
            "train acc 0.5153556568258371 loss 0.05413108466609353 \n",
            "train acc 0.5185258569447196 loss 0.05383815654484746 \n",
            "train acc 0.5216960570636021 loss 0.05352819524670555 \n",
            "train acc 0.5248662571824846 loss 0.053223188212238405 \n",
            "train acc 0.5280364573013672 loss 0.052913807795077185 \n",
            "train acc 0.5312066574202496 loss 0.05265629331492271 \n",
            "train acc 0.5343768575391321 loss 0.052355455949137326 \n",
            "train acc 0.5375470576580146 loss 0.052066741948121574 \n",
            "train acc 0.5407172577768972 loss 0.0517775891924067 \n",
            "train acc 0.5436893203883495 loss 0.0525379407085109 \n",
            "train acc 0.546859520507232 loss 0.05224329498031043 \n",
            "train acc 0.5500297206261145 loss 0.051961065637442516 \n",
            "train acc 0.553199920744997 loss 0.05168668280512168 \n",
            "train acc 0.5563701208638795 loss 0.051437343016536276 \n",
            "train acc 0.559540320982762 loss 0.051157618326839974 \n",
            "train acc 0.5627105211016445 loss 0.050882780518602684 \n",
            "train acc 0.565880721220527 loss 0.05061304181954407 \n",
            "train acc 0.5690509213394095 loss 0.05034302753379701 \n",
            "train acc 0.5722211214582921 loss 0.05009698527680628 \n",
            "train acc 0.574796909054884 loss 0.052396125948175526 \n",
            "train acc 0.5777689716663365 loss 0.05244312882266083 \n",
            "train acc 0.5809391717852189 loss 0.05219902070232928 \n",
            "train acc 0.5841093719041014 loss 0.05192778366188305 \n",
            "train acc 0.5872795720229839 loss 0.0516551609217323 \n",
            "train acc 0.5904497721418664 loss 0.05139244839311306 \n",
            "train acc 0.5934218347533188 loss 0.05224158001885945 \n",
            "train acc 0.5965920348722012 loss 0.05197289298777347 \n",
            "train acc 0.5997622349910838 loss 0.05171157536430352 \n",
            "train acc 0.6029324351099663 loss 0.051449025935587506 \n",
            "train acc 0.6061026352288488 loss 0.05133807015654478 \n",
            "train acc 0.6092728353477312 loss 0.0511519148065231 \n",
            "train acc 0.6124430354666138 loss 0.05090091089695473 \n",
            "train acc 0.6154150980780662 loss 0.051723912142295166 \n",
            "train acc 0.6185852981969486 loss 0.05146842014022181 \n",
            "train acc 0.6217554983158311 loss 0.05121521263391519 \n",
            "train acc 0.6249256984347137 loss 0.050967384352406955 \n",
            "train acc 0.6280958985535962 loss 0.05072184037608303 \n",
            "train acc 0.6312660986724786 loss 0.05047564440272002 \n",
            "train acc 0.6344362987913612 loss 0.050230477534846996 \n",
            "train acc 0.6374083614028135 loss 0.05101704889395977 \n",
            "train acc 0.640578561521696 loss 0.050775069218958024 \n",
            "train acc 0.6437487616405785 loss 0.05053552518936679 \n",
            "train acc 0.6467208242520309 loss 0.051933014705486974 \n",
            "train acc 0.6498910243709134 loss 0.051711305567061154 \n",
            "train acc 0.6530612244897959 loss 0.05154112916585708 \n",
            "train acc 0.6562314246086783 loss 0.051301579637774486 \n",
            "train acc 0.6594016247275609 loss 0.05106365090976491 \n",
            "train acc 0.6625718248464434 loss 0.05083029222743858 \n",
            "train acc 0.6655438874578957 loss 0.051786420886249594 \n",
            "train acc 0.6687140875767782 loss 0.051583366944818 \n",
            "train acc 0.6718842876956608 loss 0.05135961936673063 \n",
            "train acc 0.674658212799683 loss 0.053243991532697155 \n",
            "train acc 0.6778284129185654 loss 0.05301162834394045 \n",
            "train acc 0.6809986130374479 loss 0.05277247546420116 \n",
            "train acc 0.6841688131563305 loss 0.05276080321937323 \n",
            "train acc 0.687339013275213 loss 0.05252374136153693 \n",
            "train acc 0.6905092133940954 loss 0.05229709825712378 \n",
            "train acc 0.693679413512978 loss 0.052109130241316616 \n",
            "train acc 0.6968496136318605 loss 0.051878937516954514 \n",
            "train acc 0.6998216762433128 loss 0.05353701829049636 \n",
            "train acc 0.7029918763621953 loss 0.05330846659839153 \n",
            "train acc 0.7061620764810779 loss 0.05308221051659007 \n",
            "train acc 0.7093322765999603 loss 0.05285281279085344 \n",
            "train acc 0.7125024767188428 loss 0.052626146168015045 \n",
            "train acc 0.7154745393302951 loss 0.05378812629684773 \n",
            "train acc 0.7186447394491777 loss 0.05356268997294018 \n",
            "train acc 0.7218149395680602 loss 0.05333546144551158 \n",
            "train acc 0.7249851396869427 loss 0.05311934261508527 \n",
            "train acc 0.727957202298395 loss 0.053657257858995555 \n",
            "train acc 0.7309292649098474 loss 0.0545439182389647 \n",
            "train acc 0.7339013275212998 loss 0.05477441825279768 \n",
            "train acc 0.7370715276401822 loss 0.054554090049350634 \n",
            "train acc 0.7402417277590647 loss 0.05433601167844805 \n",
            "train acc 0.7434119278779473 loss 0.05411300017349735 \n",
            "train acc 0.7465821279968298 loss 0.05390106549166985 \n",
            "train acc 0.7497523281157122 loss 0.05377474163348476 \n",
            "train acc 0.7529225282345948 loss 0.053572295075166525 \n",
            "train acc 0.7558945908460472 loss 0.05358081012733342 \n",
            "train acc 0.7590647909649296 loss 0.053384195932727536 \n",
            "train acc 0.762036853576382 loss 0.05365134240891479 \n",
            "train acc 0.7652070536952644 loss 0.053438727463576564 \n",
            "train acc 0.768377253814147 loss 0.05322904134740886 \n",
            "train acc 0.7715474539330295 loss 0.05302967478498545 \n",
            "train acc 0.774717654051912 loss 0.05283354916478387 \n",
            "train acc 0.7778878541707945 loss 0.052634538284102715 \n",
            "train acc 0.781058054289677 loss 0.05244147590920329 \n",
            "train acc 0.7842282544085595 loss 0.052241633357702794 \n",
            "train acc 0.787398454527442 loss 0.05204023381589823 \n",
            "train acc 0.7905686546463245 loss 0.051842316006413355 \n",
            "train acc 0.793738854765207 loss 0.05165820401831068 \n",
            "train acc 0.7967109173766593 loss 0.052111603614107216 \n",
            "train acc 0.7998811174955419 loss 0.051913196971327125 \n",
            "train acc 0.8030513176144244 loss 0.05171755163799899 \n",
            "train acc 0.8062215177333069 loss 0.05152417472094413 \n",
            "train acc 0.8093917178521893 loss 0.051342298697435715 \n",
            "train acc 0.8125619179710719 loss 0.051149118036175 \n",
            "train acc 0.8157321180899544 loss 0.05096036086030219 \n",
            "train acc 0.8187041807014067 loss 0.05210751902404462 \n",
            "train acc 0.8218743808202892 loss 0.051917235745017914 \n",
            "train acc 0.8250445809391718 loss 0.05172794985992368 \n",
            "train acc 0.8282147810580542 loss 0.051540480604623705 \n",
            "train acc 0.8313849811769367 loss 0.051355754537770484 \n",
            "train acc 0.8343570437883892 loss 0.05284355386999202 \n",
            "train acc 0.8375272439072716 loss 0.05266249571091134 \n",
            "train acc 0.8406974440261541 loss 0.05248415966381468 \n",
            "train acc 0.8436695066376064 loss 0.052774822578713715 \n",
            "train acc 0.846839706756489 loss 0.052583617609504585 \n",
            "train acc 0.8500099068753715 loss 0.05239256591106395 \n",
            "train acc 0.853180106994254 loss 0.05228220594188901 \n",
            "train acc 0.8563503071131364 loss 0.0520968084473394 \n",
            "train acc 0.8593223697245889 loss 0.05300472093830732 \n",
            "train acc 0.8624925698434713 loss 0.05281806320367057 \n",
            "train acc 0.8654646324549237 loss 0.052934348652986574 \n",
            "train acc 0.8686348325738061 loss 0.05277054474216705 \n",
            "train acc 0.8718050326926887 loss 0.05258538740132952 \n",
            "train acc 0.8749752328115712 loss 0.05240117146708404 \n",
            "train acc 0.8781454329304537 loss 0.052220295344519156 \n",
            "train acc 0.8813156330493362 loss 0.05204752419789566 \n",
            "train acc 0.8844858331682187 loss 0.05186652771054106 \n",
            "train acc 0.8876560332871012 loss 0.05172849557498588 \n",
            "train acc 0.8906280958985535 loss 0.05252617582670625 \n",
            "train acc 0.8937982960174361 loss 0.05234468189170714 \n",
            "train acc 0.8969684961363186 loss 0.052167711707679004 \n",
            "train acc 0.900138696255201 loss 0.05199773406760263 \n",
            "train acc 0.9029126213592232 loss 0.05291052831854078 \n",
            "train acc 0.9060828214781057 loss 0.05273244913731669 \n",
            "train acc 0.9092530215969883 loss 0.052559123995403446 \n",
            "train acc 0.9124232217158708 loss 0.05240829354024505 \n",
            "train acc 0.9155934218347532 loss 0.052234015844717825 \n",
            "train acc 0.9187636219536358 loss 0.052059450820737777 \n",
            "train acc 0.9219338220725183 loss 0.05189122104330651 \n",
            "train acc 0.9251040221914008 loss 0.05171805484939329 \n",
            "train acc 0.9282742223102833 loss 0.051552219817693964 \n",
            "train acc 0.9314444224291658 loss 0.05138163554525915 \n",
            "train acc 0.9346146225480483 loss 0.05121885778002416 \n",
            "train acc 0.9377848226669308 loss 0.05106032131006941 \n",
            "train acc 0.9409550227858133 loss 0.0509087252500235 \n",
            "train acc 0.9441252229046958 loss 0.05078529893954142 \n",
            "train acc 0.9472954230235783 loss 0.0506252550424719 \n",
            "train acc 0.9504656231424609 loss 0.050493113920215126 \n",
            "train acc 0.9536358232613433 loss 0.050334705588728434 \n",
            "train acc 0.9568060233802258 loss 0.050211268372900686 \n",
            "train acc 0.9599762234991084 loss 0.050052925650825184 \n",
            "train acc 0.9631464236179909 loss 0.04989632139891059 \n",
            "train acc 0.9663166237368733 loss 0.04973972871376678 \n",
            "train acc 0.9694868238557558 loss 0.04958491026378808 \n",
            "train acc 0.9726570239746384 loss 0.04942930187884766 \n",
            "train acc 0.9758272240935209 loss 0.04927623798908033 \n",
            "train acc 0.9789974242124033 loss 0.049124389054467935 \n",
            "train acc 0.9821676243312859 loss 0.04897373961062307 \n",
            "train acc 0.9853378244501684 loss 0.04884348058230465 \n",
            "train acc 0.9867247870021795 loss 0.048690941956563254 \n",
            "Train loss 0.048690941956563254 accuracy 0.9867247870021795 accuracy1 0.9867247870021795\n",
            "val acc 0.02377179080824089 loss 0.16190564632415771 \n",
            "val acc 0.049128367670364506 loss 0.08153814828256145 \n",
            "val acc 0.07448494453248812 loss 0.06481656311855961 \n",
            "val acc 0.09984152139461173 loss 0.04888377885799855 \n",
            "val acc 0.12519809825673534 loss 0.03927942623849958 \n",
            "val acc 0.15055467511885895 loss 0.03693029375669236 \n",
            "val acc 0.17591125198098256 loss 0.03175650504584025 \n",
            "val acc 0.19968304278922347 loss 0.06300053530867444 \n",
            "val acc 0.22345483359746435 loss 0.11460831901705307 \n",
            "val acc 0.24564183835182252 loss 0.15018842942663468 \n",
            "val acc 0.27099841521394613 loss 0.13670129807856443 \n",
            "val acc 0.29635499207606975 loss 0.12546806511333367 \n",
            "val acc 0.32171156893819336 loss 0.11698543739308101 \n",
            "val acc 0.34548335974643424 loss 0.11196112100982905 \n",
            "val acc 0.3692551505546751 loss 0.11659665306797251 \n",
            "val acc 0.3914421553090333 loss 0.13445644639796228 \n",
            "val acc 0.4167987321711569 loss 0.12754395922221354 \n",
            "val acc 0.44215530903328054 loss 0.12057226082995637 \n",
            "val acc 0.4659270998415214 loss 0.13133617935665443 \n",
            "val acc 0.48811410459587956 loss 0.14535740627034102 \n",
            "val acc 0.5118858954041204 loss 0.1644057036693474 \n",
            "val acc 0.5372424722662441 loss 0.1569555672044358 \n",
            "val acc 0.5625990491283677 loss 0.15015863438400076 \n",
            "val acc 0.5879556259904913 loss 0.14394671214783253 \n",
            "val acc 0.6085578446909667 loss 0.1616366266552359 \n",
            "val acc 0.6339144215530903 loss 0.15655587564329976 \n",
            "val acc 0.6576862123613313 loss 0.16443712023707727 \n",
            "val acc 0.6814580031695722 loss 0.18017944754267642 \n",
            "val acc 0.7036450079239303 loss 0.20866735697852384 \n",
            "val acc 0.7290015847860539 loss 0.2019113472274815 \n",
            "val acc 0.7543581616481775 loss 0.19587782312995725 \n",
            "val acc 0.7797147385103012 loss 0.1897827032407804 \n",
            "val acc 0.803486529318542 loss 0.18586910104336726 \n",
            "val acc 0.8288431061806656 loss 0.18076202958507245 \n",
            "val acc 0.8541996830427893 loss 0.17562214416351968 \n",
            "val acc 0.8795562599049128 loss 0.17076068615036397 \n",
            "val acc 0.9049128367670365 loss 0.16734509890257204 \n",
            "val acc 0.9270998415213947 loss 0.17508077185515217 \n",
            "val acc 0.9524564183835182 loss 0.17062050270406195 \n",
            "val acc 0.9619651347068147 loss 0.17391654787643346 \n",
            "Val   loss 0.17391654787643346 accuracy 0.9619651347068147 accuracy1 0.9619651347068146\n",
            "Epoch 5/5\n",
            "----------\n",
            "train acc 0.0031702001188825043 loss 0.002083017723634839 \n",
            "train acc 0.006340400237765009 loss 0.002868391224183142 \n",
            "train acc 0.009510600356647513 loss 0.002530718765531977 \n",
            "train acc 0.012680800475530017 loss 0.0020834565948462114 \n",
            "train acc 0.015652863086982365 loss 0.05717926508514211 \n",
            "train acc 0.018823063205864868 loss 0.04782648313751755 \n",
            "train acc 0.021993263324747374 loss 0.04117939006703507 \n",
            "train acc 0.025163463443629876 loss 0.03613157913059695 \n",
            "train acc 0.028333663562512382 loss 0.03225866541551012 \n",
            "train acc 0.031503863681394885 loss 0.029082628368632867 \n",
            "train acc 0.03467406380027739 loss 0.026641021826600827 \n",
            "train acc 0.0378442639191599 loss 0.024472255191843335 \n",
            "train acc 0.041014464038042396 loss 0.022636886498031136 \n",
            "train acc 0.0441846641569249 loss 0.021106536558363587 \n",
            "train acc 0.04735486427580741 loss 0.019762296435267977 \n",
            "train acc 0.05032692688725975 loss 0.051400086802459555 \n",
            "train acc 0.05349712700614226 loss 0.05075545714798329 \n",
            "train acc 0.056667327125024765 loss 0.0479817857590711 \n",
            "train acc 0.05983752724390727 loss 0.045505894069241264 \n",
            "train acc 0.06300772736278977 loss 0.04332413029915187 \n",
            "train acc 0.06617792748167228 loss 0.04129510509249355 \n",
            "train acc 0.06934812760055478 loss 0.039465462032239884 \n",
            "train acc 0.07251832771943728 loss 0.03783640185998672 \n",
            "train acc 0.0756885278383198 loss 0.036594770785692766 \n",
            "train acc 0.0788587279572023 loss 0.03635198110248893 \n",
            "train acc 0.08202892807608479 loss 0.03514251843220196 \n",
            "train acc 0.0851991281949673 loss 0.033897168835176635 \n",
            "train acc 0.0883693283138498 loss 0.03279355424872067 \n",
            "train acc 0.09153952843273232 loss 0.03262166913743291 \n",
            "train acc 0.09470972855161482 loss 0.031567496484300746 \n",
            "train acc 0.09787992867049732 loss 0.030611126739982394 \n",
            "train acc 0.10105012878937983 loss 0.029677156397156068 \n",
            "train acc 0.10402219140083217 loss 0.030232298930500154 \n",
            "train acc 0.10719239151971467 loss 0.02939546699053608 \n",
            "train acc 0.11036259163859719 loss 0.028591786915369864 \n",
            "train acc 0.11353279175747968 loss 0.027829222259849 \n",
            "train acc 0.11670299187636218 loss 0.02710227369381165 \n",
            "train acc 0.1198731919952447 loss 0.02643383827709936 \n",
            "train acc 0.1230433921141272 loss 0.025779084559195697 \n",
            "train acc 0.1262135922330097 loss 0.025150583188224118 \n",
            "train acc 0.1293837923518922 loss 0.02457123827160813 \n",
            "train acc 0.1325539924707747 loss 0.02401235727787328 \n",
            "train acc 0.1357241925896572 loss 0.02346846657983829 \n",
            "train acc 0.13889439270853973 loss 0.02294887487369124 \n",
            "train acc 0.14206459282742223 loss 0.022460867442552827 \n",
            "train acc 0.14523479294630473 loss 0.022034417588577566 \n",
            "train acc 0.14840499306518723 loss 0.02160708591010065 \n",
            "train acc 0.15157519318406973 loss 0.021214588619235048 \n",
            "train acc 0.15474539330295223 loss 0.020808345953012074 \n",
            "train acc 0.15791559342183475 loss 0.02041896492592059 \n",
            "train acc 0.16108579354071725 loss 0.02003004962656026 \n",
            "train acc 0.16425599365959975 loss 0.019659637120919518 \n",
            "train acc 0.16742619377848225 loss 0.01930712280632436 \n",
            "train acc 0.17059639389736475 loss 0.018977532108288467 \n",
            "train acc 0.17376659401624728 loss 0.01864572832022201 \n",
            "train acc 0.17693679413512978 loss 0.018740615449912314 \n",
            "train acc 0.18010699425401228 loss 0.01861077289734232 \n",
            "train acc 0.18327719437289477 loss 0.018319580962897503 \n",
            "train acc 0.18644739449177727 loss 0.018025380296837853 \n",
            "train acc 0.1896175946106598 loss 0.01774950561230071 \n",
            "train acc 0.1927877947295423 loss 0.01747025276217457 \n",
            "train acc 0.1959579948484248 loss 0.017206657772711027 \n",
            "train acc 0.1991281949673073 loss 0.01696831560120844 \n",
            "train acc 0.2022983950861898 loss 0.016734922262003238 \n",
            "train acc 0.2054685952050723 loss 0.016493226463297525 \n",
            "train acc 0.20863879532395482 loss 0.01625873439391426 \n",
            "train acc 0.21180899544283732 loss 0.0160499986968309 \n",
            "train acc 0.21497919556171982 loss 0.01583704485314871 \n",
            "train acc 0.21814939568060232 loss 0.015626419937082876 \n",
            "train acc 0.22131959579948482 loss 0.015424010120165933 \n",
            "train acc 0.22448979591836735 loss 0.015218711111360085 \n",
            "train acc 0.22765999603724985 loss 0.015022111609545795 \n",
            "train acc 0.23083019615613234 loss 0.014837353298128008 \n",
            "train acc 0.23400039627501484 loss 0.014647698944679939 \n",
            "train acc 0.23717059639389734 loss 0.014467791004572064 \n",
            "train acc 0.24034079651277987 loss 0.014287062752900008 \n",
            "train acc 0.24351099663166237 loss 0.014123553348941028 \n",
            "train acc 0.2464830592431147 loss 0.01956396966572636 \n",
            "train acc 0.24965325936199723 loss 0.01932732224732614 \n",
            "train acc 0.25262532197344956 loss 0.021921037616994 \n",
            "train acc 0.2557955220923321 loss 0.02167017674461628 \n",
            "train acc 0.25896572221121456 loss 0.021444224254881235 \n",
            "train acc 0.2621359223300971 loss 0.021254420002041585 \n",
            "train acc 0.26530612244897955 loss 0.021061883276810738 \n",
            "train acc 0.2684763225678621 loss 0.020894590975470185 \n",
            "train acc 0.2716465226867446 loss 0.020686685014922216 \n",
            "train acc 0.2748167228056271 loss 0.020457148916575116 \n",
            "train acc 0.2779869229245096 loss 0.020228825861439956 \n",
            "train acc 0.2811571230433921 loss 0.020013355919463415 \n",
            "train acc 0.2843273231622746 loss 0.019797729276534583 \n",
            "train acc 0.28749752328115713 loss 0.019584281209173132 \n",
            "train acc 0.2906677234000396 loss 0.019383115384224093 \n",
            "train acc 0.29383792351892213 loss 0.019182103798381724 \n",
            "train acc 0.2970081236378046 loss 0.018983787858544137 \n",
            "train acc 0.3001783237566871 loss 0.018793338557129332 \n",
            "train acc 0.30334852387556965 loss 0.018606744415895566 \n",
            "train acc 0.3065187239944521 loss 0.018426902570879356 \n",
            "train acc 0.30968892411333465 loss 0.018244169348653652 \n",
            "train acc 0.3128591242322171 loss 0.018074265736769037 \n",
            "train acc 0.31602932435109965 loss 0.01790340172155993 \n",
            "train acc 0.3191995244699822 loss 0.01773212131171598 \n",
            "train acc 0.3221715870814345 loss 0.020764584090120122 \n",
            "train acc 0.32514364969288684 loss 0.02107323865599487 \n",
            "train acc 0.32831384981176936 loss 0.02090143583028783 \n",
            "train acc 0.33148404993065184 loss 0.020720580015941303 \n",
            "train acc 0.33465425004953436 loss 0.020539074653774976 \n",
            "train acc 0.3378244501684169 loss 0.02037967036551117 \n",
            "train acc 0.34099465028729936 loss 0.02020084527639146 \n",
            "train acc 0.3441648504061819 loss 0.02004360925986158 \n",
            "train acc 0.34733505052506436 loss 0.019881925113573246 \n",
            "train acc 0.3505052506439469 loss 0.019715117588209256 \n",
            "train acc 0.35367545076282936 loss 0.019567884338227617 \n",
            "train acc 0.3568456508817119 loss 0.01941193328769589 \n",
            "train acc 0.3598177134931642 loss 0.022243359551730305 \n",
            "train acc 0.36298791361204674 loss 0.022060188333979686 \n",
            "train acc 0.36615811373092927 loss 0.021887138326088337 \n",
            "train acc 0.36932831384981174 loss 0.021712426354652908 \n",
            "train acc 0.37230037646126407 loss 0.023359862333548382 \n",
            "train acc 0.3754705765801466 loss 0.02317981798756484 \n",
            "train acc 0.3786407766990291 loss 0.023080929512070726 \n",
            "train acc 0.3818109768179116 loss 0.02290147401787887 \n",
            "train acc 0.3849811769367941 loss 0.02272380448636393 \n",
            "train acc 0.38795323954824645 loss 0.023016419321084697 \n",
            "train acc 0.391123439667129 loss 0.022839713189891735 \n",
            "train acc 0.3940955022785813 loss 0.024368426987668498 \n",
            "train acc 0.39726570239746384 loss 0.024178779701330876 \n",
            "train acc 0.4004359025163463 loss 0.02399684510108516 \n",
            "train acc 0.40360610263522884 loss 0.023815122455516757 \n",
            "train acc 0.40657816524668117 loss 0.025009616885231376 \n",
            "train acc 0.4097483653655637 loss 0.02482551220746245 \n",
            "train acc 0.41291856548444617 loss 0.024645659970146255 \n",
            "train acc 0.41589062809589855 loss 0.02518788508953576 \n",
            "train acc 0.419060828214781 loss 0.025003290709144876 \n",
            "train acc 0.42223102833366355 loss 0.024822909418196388 \n",
            "train acc 0.425401228452546 loss 0.02464897470846255 \n",
            "train acc 0.42857142857142855 loss 0.024477846871056554 \n",
            "train acc 0.4317416286903111 loss 0.024308740860913426 \n",
            "train acc 0.4347136913017634 loss 0.026750406695140082 \n",
            "train acc 0.43788389142064593 loss 0.026566807410333026 \n",
            "train acc 0.4410540915395284 loss 0.026385809275961947 \n",
            "train acc 0.44422429165841093 loss 0.02620890218758764 \n",
            "train acc 0.4473944917772934 loss 0.026034116433666113 \n",
            "train acc 0.4503665543887458 loss 0.028161141529908874 \n",
            "train acc 0.45353675450762826 loss 0.027974243347974454 \n",
            "train acc 0.4567069546265108 loss 0.027789718879523268 \n",
            "train acc 0.45987715474539326 loss 0.027606319384338423 \n",
            "train acc 0.4630473548642758 loss 0.02742678287924508 \n",
            "train acc 0.4662175549831583 loss 0.027251026937085192 \n",
            "train acc 0.46918961759461064 loss 0.030248299557692215 \n",
            "train acc 0.4723598177134931 loss 0.03006860887107905 \n",
            "train acc 0.47553001783237564 loss 0.029883478594896082 \n",
            "train acc 0.47870021795125817 loss 0.029695838040520596 \n",
            "train acc 0.48187041807014064 loss 0.02951412867496758 \n",
            "train acc 0.48504061818902316 loss 0.029331045963252627 \n",
            "train acc 0.48821081830790564 loss 0.02914815177453772 \n",
            "train acc 0.49138101842678816 loss 0.028973126536397897 \n",
            "train acc 0.4945512185456707 loss 0.028822734892838427 \n",
            "train acc 0.49772141866455316 loss 0.028660149860403595 \n",
            "train acc 0.5008916187834357 loss 0.028520311166147214 \n",
            "train acc 0.5040618189023182 loss 0.028357067449724126 \n",
            "train acc 0.5072320190212007 loss 0.02819196886751043 \n",
            "train acc 0.5104022191400832 loss 0.02802457119626378 \n",
            "train acc 0.5135724192589657 loss 0.02786031715158356 \n",
            "train acc 0.5167426193778482 loss 0.02770005622484704 \n",
            "train acc 0.5197146819893006 loss 0.02801344053145978 \n",
            "train acc 0.5228848821081831 loss 0.027858005585015683 \n",
            "train acc 0.5260550822270655 loss 0.02769747904720047 \n",
            "train acc 0.529225282345948 loss 0.02754306637178247 \n",
            "train acc 0.5323954824648306 loss 0.027383621520129964 \n",
            "train acc 0.5355656825837131 loss 0.027231049703208127 \n",
            "train acc 0.5387358827025955 loss 0.027077740744370367 \n",
            "train acc 0.5419060828214781 loss 0.026930696199489703 \n",
            "train acc 0.5450762829403606 loss 0.026814844463164567 \n",
            "train acc 0.5482464830592431 loss 0.02668766715231314 \n",
            "train acc 0.5514166831781256 loss 0.026539802463625424 \n",
            "train acc 0.5545868832970081 loss 0.026403384690803996 \n",
            "train acc 0.5577570834158906 loss 0.026263398809738638 \n",
            "train acc 0.5609272835347731 loss 0.02612358128035237 \n",
            "train acc 0.5640974836536556 loss 0.0259848088151242 \n",
            "train acc 0.5672676837725381 loss 0.02584726388271924 \n",
            "train acc 0.5704378838914206 loss 0.025712891557872635 \n",
            "train acc 0.5736080840103032 loss 0.025577644387849832 \n",
            "train acc 0.5767782841291856 loss 0.025442953224839703 \n",
            "train acc 0.5799484842480681 loss 0.025411894077557878 \n",
            "train acc 0.5831186843669506 loss 0.025349692077331898 \n",
            "train acc 0.5862888844858332 loss 0.025226770119862983 \n",
            "train acc 0.5894590846047156 loss 0.025098220199467315 \n",
            "train acc 0.5926292847235981 loss 0.024967897378256238 \n",
            "train acc 0.5957994848424807 loss 0.024839886512992924 \n",
            "train acc 0.5989696849613632 loss 0.024723622565139332 \n",
            "train acc 0.6021398850802456 loss 0.02459764531316816 \n",
            "train acc 0.6053100851991282 loss 0.024477536477358324 \n",
            "train acc 0.6084802853180107 loss 0.024355087972305223 \n",
            "train acc 0.6116504854368932 loss 0.024304194968900662 \n",
            "train acc 0.6148206855557756 loss 0.024185340678564107 \n",
            "train acc 0.6179908856746582 loss 0.024069740447216922 \n",
            "train acc 0.6209629482861105 loss 0.02495681883150896 \n",
            "train acc 0.624133148404993 loss 0.024834315234770518 \n",
            "train acc 0.6273033485238755 loss 0.02471398923943292 \n",
            "train acc 0.6304735486427581 loss 0.02459995777971926 \n",
            "train acc 0.6336437487616405 loss 0.02448501070339895 \n",
            "train acc 0.636813948880523 loss 0.024367850270821934 \n",
            "train acc 0.6399841489994056 loss 0.024251073482303005 \n",
            "train acc 0.6431543491182881 loss 0.024240618747831368 \n",
            "train acc 0.6463245492371705 loss 0.024130347900223215 \n",
            "train acc 0.6494947493560531 loss 0.024018583148327947 \n",
            "train acc 0.6524668119675054 loss 0.02552797519171651 \n",
            "train acc 0.6556370120863879 loss 0.025444072060473936 \n",
            "train acc 0.6588072122052704 loss 0.02533125363189491 \n",
            "train acc 0.661977412324153 loss 0.025216005412013538 \n",
            "train acc 0.6651476124430354 loss 0.02510036679712495 \n",
            "train acc 0.6683178125619179 loss 0.02498796633496894 \n",
            "train acc 0.6712898751733702 loss 0.025300124042395265 \n",
            "train acc 0.6744600752922528 loss 0.025187356505628946 \n",
            "train acc 0.6776302754111353 loss 0.025079067149280765 \n",
            "train acc 0.6806023380225876 loss 0.02663906269824818 \n",
            "train acc 0.6837725381414701 loss 0.02652369945485484 \n",
            "train acc 0.6869427382603527 loss 0.026404297798621006 \n",
            "train acc 0.6901129383792352 loss 0.02630929458903927 \n",
            "train acc 0.6932831384981176 loss 0.02619215033310783 \n",
            "train acc 0.6964533386170002 loss 0.02608305444807653 \n",
            "train acc 0.6996235387358827 loss 0.025976335982606444 \n",
            "train acc 0.7027937388547651 loss 0.025863853136402232 \n",
            "train acc 0.7057658014662175 loss 0.02728162626005672 \n",
            "train acc 0.7089360015851001 loss 0.02716464288209358 \n",
            "train acc 0.7121062017039825 loss 0.02705209067519404 \n",
            "train acc 0.715276401822865 loss 0.026935971151634288 \n",
            "train acc 0.7184466019417476 loss 0.026821000146778635 \n",
            "train acc 0.7214186645531999 loss 0.028079650812811833 \n",
            "train acc 0.7245888646720824 loss 0.027961722004313388 \n",
            "train acc 0.7277590647909649 loss 0.027844306825507974 \n",
            "train acc 0.7309292649098474 loss 0.0277295886461898 \n",
            "train acc 0.7340994650287299 loss 0.027617633343367465 \n",
            "train acc 0.7370715276401822 loss 0.028565447364210423 \n",
            "train acc 0.7402417277590647 loss 0.02848365177719933 \n",
            "train acc 0.7434119278779473 loss 0.028368157143847647 \n",
            "train acc 0.7465821279968298 loss 0.028251510993837877 \n",
            "train acc 0.7497523281157122 loss 0.028135783092961098 \n",
            "train acc 0.7529225282345948 loss 0.028025185616387865 \n",
            "train acc 0.7560927283534773 loss 0.027944580819773062 \n",
            "train acc 0.7592629284723598 loss 0.027842517025479893 \n",
            "train acc 0.7624331285912422 loss 0.027755179179929514 \n",
            "train acc 0.7656033287101248 loss 0.027648618009440608 \n",
            "train acc 0.7687735288290073 loss 0.02755460876279194 \n",
            "train acc 0.7719437289478898 loss 0.027446224752989388 \n",
            "train acc 0.7751139290667723 loss 0.02733945268609966 \n",
            "train acc 0.7782841291856548 loss 0.02723647835289754 \n",
            "train acc 0.7814543293045373 loss 0.027135631726372185 \n",
            "train acc 0.7846245294234198 loss 0.02703545090284696 \n",
            "train acc 0.7877947295423023 loss 0.02694009802083019 \n",
            "train acc 0.7909649296611848 loss 0.0268421957295516 \n",
            "train acc 0.7941351297800673 loss 0.026739943533371727 \n",
            "train acc 0.7973053298989499 loss 0.026639874741686286 \n",
            "train acc 0.8004755300178323 loss 0.02654229190191859 \n",
            "train acc 0.8036457301367148 loss 0.02651831129915081 \n",
            "train acc 0.8068159302555974 loss 0.02642107696203766 \n",
            "train acc 0.8099861303744799 loss 0.02632163664480537 \n",
            "train acc 0.8131563304933623 loss 0.026232861102104744 \n",
            "train acc 0.8163265306122448 loss 0.02613947560085239 \n",
            "train acc 0.8194967307311274 loss 0.0260432047345183 \n",
            "train acc 0.8226669308500099 loss 0.025949402098205846 \n",
            "train acc 0.8256389934614622 loss 0.026481299223542705 \n",
            "train acc 0.8288091935803447 loss 0.02638507495023852 \n",
            "train acc 0.8319793936992272 loss 0.026290151950392596 \n",
            "train acc 0.8351495938181097 loss 0.026196117428206083 \n",
            "train acc 0.8383197939369922 loss 0.026102023405006287 \n",
            "train acc 0.8412918565484446 loss 0.026885615469131808 \n",
            "train acc 0.8444620566673271 loss 0.026797094829871367 \n",
            "train acc 0.8476322567862096 loss 0.02670536936725123 \n",
            "train acc 0.850802456905092 loss 0.026614114762115705 \n",
            "train acc 0.8539726570239746 loss 0.02651908499933178 \n",
            "train acc 0.8571428571428571 loss 0.026423659005612374 \n",
            "train acc 0.8603130572617396 loss 0.02633488919846492 \n",
            "train acc 0.8634832573806221 loss 0.026245399537036557 \n",
            "train acc 0.8664553199920745 loss 0.027448523466962135 \n",
            "train acc 0.869625520110957 loss 0.027354795300883437 \n",
            "train acc 0.8727957202298394 loss 0.027264694886560307 \n",
            "train acc 0.875965920348722 loss 0.0271725566958692 \n",
            "train acc 0.8791361204676045 loss 0.027077567360241226 \n",
            "train acc 0.882306320586487 loss 0.026983246825173928 \n",
            "train acc 0.8854765207053695 loss 0.02689057738208245 \n",
            "train acc 0.888646720824252 loss 0.026804098591818606 \n",
            "train acc 0.8918169209431345 loss 0.026711766259944685 \n",
            "train acc 0.8949871210620169 loss 0.026619640983945304 \n",
            "train acc 0.8979591836734694 loss 0.026993077318490433 \n",
            "train acc 0.9011293837923519 loss 0.026900313986840632 \n",
            "train acc 0.9042995839112343 loss 0.02681131869522475 \n",
            "train acc 0.9074697840301168 loss 0.0267218458358103 \n",
            "train acc 0.9104418466415692 loss 0.026977003821607674 \n",
            "train acc 0.9136120467604517 loss 0.02688707329627881 \n",
            "train acc 0.9167822468793342 loss 0.02679946545031021 \n",
            "train acc 0.9199524469982168 loss 0.02680912644557307 \n",
            "train acc 0.9231226471170992 loss 0.026720311994598414 \n",
            "train acc 0.9262928472359817 loss 0.02663207444713568 \n",
            "train acc 0.9294630473548642 loss 0.02654484758869808 \n",
            "train acc 0.9326332474737468 loss 0.026457313988676352 \n",
            "train acc 0.9358034475926292 loss 0.026372065313057722 \n",
            "train acc 0.9389736477115117 loss 0.026284801702529986 \n",
            "train acc 0.9421438478303943 loss 0.026204408211754262 \n",
            "train acc 0.9453140479492768 loss 0.026125212865008508 \n",
            "train acc 0.9484842480681592 loss 0.026043645061681702 \n",
            "train acc 0.9516544481870417 loss 0.02596347937574348 \n",
            "train acc 0.9548246483059243 loss 0.025880770430736482 \n",
            "train acc 0.9579948484248068 loss 0.025805545795695873 \n",
            "train acc 0.9611650485436892 loss 0.025727446102747145 \n",
            "train acc 0.9643352486625718 loss 0.025645054818691802 \n",
            "train acc 0.9675054487814543 loss 0.025565123136313813 \n",
            "train acc 0.9706756489003368 loss 0.02548496705465179 \n",
            "train acc 0.9738458490192193 loss 0.02540677876905297 \n",
            "train acc 0.9770160491381018 loss 0.025327454016059486 \n",
            "train acc 0.9801862492569843 loss 0.025248308305481113 \n",
            "train acc 0.9833564493758667 loss 0.02517374037070579 \n",
            "train acc 0.9865266494947493 loss 0.025098112689910648 \n",
            "train acc 0.9896968496136318 loss 0.025024908816402593 \n",
            "train acc 0.9928670497325143 loss 0.024951209577580263 \n",
            "train acc 0.9942540122845254 loss 0.024874239341857625 \n",
            "Train loss 0.024874239341857625 accuracy 0.9942540122845254 accuracy1 0.9942540122845255\n",
            "val acc 0.025356576862123614 loss 0.0159746240824461 \n",
            "val acc 0.05071315372424723 loss 0.008480128017254174 \n",
            "val acc 0.07448494453248812 loss 0.03877731668762863 \n",
            "val acc 0.09984152139461173 loss 0.02932410652283579 \n",
            "val acc 0.12519809825673534 loss 0.02364199113799259 \n",
            "val acc 0.15055467511885895 loss 0.02009503398827898 \n",
            "val acc 0.17591125198098256 loss 0.01733462433497022 \n",
            "val acc 0.19809825673534073 loss 0.049937759766180534 \n",
            "val acc 0.22187004754358164 loss 0.10313631201602726 \n",
            "val acc 0.24405705229793978 loss 0.1493597665859852 \n",
            "val acc 0.2694136291600634 loss 0.13592991071478042 \n",
            "val acc 0.294770206022187 loss 0.1250524221250089 \n",
            "val acc 0.3185419968304279 loss 0.12654048762767792 \n",
            "val acc 0.3423137876386688 loss 0.12289261112372125 \n",
            "val acc 0.36450079239302696 loss 0.13444609579552586 \n",
            "val acc 0.38827258320126784 loss 0.13291758100604056 \n",
            "val acc 0.4120443740095087 loss 0.13607021979645223 \n",
            "val acc 0.43740095087163233 loss 0.12864744484411655 \n",
            "val acc 0.4611727416798732 loss 0.14260014251646536 \n",
            "val acc 0.4833597464342314 loss 0.1560670017002849 \n",
            "val acc 0.5071315372424723 loss 0.17483469122920983 \n",
            "val acc 0.5324881141045958 loss 0.16691074129035274 \n",
            "val acc 0.5578446909667195 loss 0.15968281335106282 \n",
            "val acc 0.5832012678288432 loss 0.15308039722973868 \n",
            "val acc 0.6038034865293186 loss 0.169369645959232 \n",
            "val acc 0.6291600633914421 loss 0.16292816853876083 \n",
            "val acc 0.6513470681458003 loss 0.18134088643731688 \n",
            "val acc 0.6751188589540412 loss 0.19602399633939577 \n",
            "val acc 0.6973058637083994 loss 0.22188122720048153 \n",
            "val acc 0.722662440570523 loss 0.21457541489702028 \n",
            "val acc 0.7480190174326466 loss 0.2079623881220487 \n",
            "val acc 0.7733755942947702 loss 0.20148480445277528 \n",
            "val acc 0.7971473851030111 loss 0.2045945213070478 \n",
            "val acc 0.8225039619651348 loss 0.19869962958139642 \n",
            "val acc 0.8478605388272583 loss 0.19304475746882546 \n",
            "val acc 0.873217115689382 loss 0.1876972961666373 \n",
            "val acc 0.8969889064976229 loss 0.18434542924090214 \n",
            "val acc 0.9191759112519811 loss 0.18901823405298943 \n",
            "val acc 0.9445324881141046 loss 0.1842027477090223 \n",
            "val acc 0.954041204437401 loss 0.19126175814599264 \n",
            "Val   loss 0.19126175814599264 accuracy 0.954041204437401 accuracy1 0.9540412044374009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "TcWk2mLX941J",
        "outputId": "40894853-aa48-423b-a446-18b4e62be3bf"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "Bv093PchOKnX",
        "outputId": "1f844721-f000-49d3-c3f0-c9d89f68f0c5"
      },
      "source": [
        "plt.plot(history['train_loss'], label='train loss')\n",
        "plt.plot(history['val_loss'], label='validation loss')\n",
        "plt.title('Loss history')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDwkNzlk9-st",
        "outputId": "ad464bd7-9fe5-41ef-ca85-5e07ae457794"
      },
      "source": [
        "test_acc, _ = eval_model(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device\n",
        ")\n",
        "test_acc.item()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9572107765451664"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cx17MX2as0V"
      },
      "source": [
        "\"\"\"isi_berita = '''\n",
        "Indonesia membutuhkan pendidikan sejak kecil, karena dengan adanya pendidikan orang-orang indonesia dapat lebih menemukan inovasi-inovasi terbaik dan memberikan ide-ide yang sangat menarik\n",
        "dengan perkembangan teknologi, pendidikan sangat mudah didapatkan. korona tidak dapat menjadi penghalang seseorang untuk mendapatkan pendidikan. dengan adanya hp memudahkan untuk mendapatkan pendidikan dimana saja dan kapan saja\n",
        "'''\"\"\"\n",
        "\n",
        "model = model.eval()\n",
        "preds_label = []\n",
        "encoded_review = tokenizer.encode_plus(\n",
        "  isi_berita,\n",
        "  max_length=MAX_LEN,\n",
        "  add_special_tokens=True,\n",
        "  return_token_type_ids=False,\n",
        "  padding='max_length',\n",
        "  return_attention_mask=True,\n",
        "  truncation=True,\n",
        "  return_tensors='pt',\n",
        ")\n",
        "\n",
        "input_ids = encoded_review['input_ids'].to(device)\n",
        "attention_mask = encoded_review['attention_mask'].to(device)\n",
        "\n",
        "output = model(input_ids, attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VMELGNkawDF"
      },
      "source": [
        "prob = F.softmax(output, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCxt7Qg_a1fY",
        "outputId": "4db00f8a-d0eb-44eb-82ec-9d838f8c0a55"
      },
      "source": [
        "prob"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3.3076e-04, 9.9649e-01, 4.7558e-05, 2.6333e-03, 5.0014e-04]],\n",
              "       device='cuda:0', grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnuJT4irbSnS"
      },
      "source": [
        "temp_labels=[]\n",
        "for a in range(len(prob)):\n",
        "    text = \"\"\n",
        "\n",
        "    indeks = 0\n",
        "    for b in range(len(prob[a])):\n",
        "      if(prob[a][b]>0.5):\n",
        "        indeks+=1\n",
        "        if indeks==1:\n",
        "          text += \"%s\"%(class_names[b])\n",
        "        else:\n",
        "          text += \", %s\"%(class_names[b])\n",
        "        #print(text)\n",
        "    temp_labels.append(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMgMnYtMbiOD",
        "outputId": "f5249306-a881-48bf-a793-c5f6a492d3ff"
      },
      "source": [
        "temp_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tekno']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASnr-QlZbxKy",
        "outputId": "50e24dd9-e13c-488c-81dc-25593ecf4fe6"
      },
      "source": [
        "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
        "outputs = eval_model_test_multilabel(\n",
        "  model,\n",
        "  test_data_loader,\n",
        "  loss_fn,\n",
        "  device\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC3NqISc-82M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "391eb37d-48ac-45a0-c1f0-2d2c241ca113"
      },
      "source": [
        "outputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.22659968249499798, 0.7654516640253566)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "UtnPdPSa5NO6",
        "outputId": "960475f4-e49b-4c1b-c6f1-e2d7ca751b95"
      },
      "source": [
        "test_data_loader.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-6a219880d48d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGk1Kv2xjvQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e1cc0db-80c6-4f89-c59e-cfbc7e7eee06"
      },
      "source": [
        "outputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.22659968249499798, 0.7654516640253566)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2c8rVyQ-C6q"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(F.softmax(outputs, dim=1))\n",
        "      real_values.extend(targets)\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIwZh22w7Zac"
      },
      "source": [
        "def get_predictions2(model, data_loader):\n",
        "  model = model.eval()\n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      prob = F.sigmoid(outputs)\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(prob)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQzA8719l83v"
      },
      "source": [
        "def get_predictions_text(model, text, max_len, threshold):\n",
        "  model = model.eval()\n",
        "  preds_label = []\n",
        "  encoded_review = tokenizer.encode_plus(\n",
        "    text,\n",
        "    max_length=max_len,\n",
        "    add_special_tokens=True,\n",
        "    return_token_type_ids=False,\n",
        "    padding='max_length',\n",
        "    return_attention_mask=True,\n",
        "    truncation=True,\n",
        "    return_tensors='pt',\n",
        "  )\n",
        "\n",
        "  input_ids = encoded_review['input_ids'].to(device)\n",
        "  attention_mask = encoded_review['attention_mask'].to(device)\n",
        "\n",
        "  output = model(input_ids, attention_mask)\n",
        "  #_, preds = torch.max(output, dim=1)\n",
        "  probs = F.sigmoid(output)\n",
        "  probs = probs.detach().cpu().numpy()\n",
        "  for a in range(len(probs[0])):\n",
        "    if(probs[0][a]>threshold):\n",
        "      preds_label.append(class_names[a])\n",
        "  return preds_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly6wm4bd-OYP",
        "outputId": "30d3e724-12c2-4e6e-f638-0f2ea86eb285"
      },
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3zh5-5Lc0td",
        "outputId": "bcef3718-b9ec-4eb7-f8d9-e423747a4203"
      },
      "source": [
        "print(classification_report(y_test, y_pred_probs>0.5, target_names=class_names))\n",
        "#print(classification_report(y_test, y_pred, target_names=class_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     edukasi       1.00      0.93      0.97        91\n",
            "       tekno       0.95      0.95      0.95        95\n",
            "      sports       0.99      0.99      0.99       224\n",
            "      health       0.79      0.92      0.85        36\n",
            "   lifestyle       0.94      0.94      0.94       185\n",
            "\n",
            "   micro avg       0.96      0.96      0.96       631\n",
            "   macro avg       0.93      0.95      0.94       631\n",
            "weighted avg       0.96      0.96      0.96       631\n",
            " samples avg       0.96      0.96      0.96       631\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkkJ92sH-X0u"
      },
      "source": [
        "idx = 192\n",
        "review_text = y_review_texts[idx]\n",
        "class_ids = y_test[idx]\n",
        "class_pred = y_pred[idx]\n",
        "predprobs = y_pred_probs[idx]\n",
        "pred_df = pd.DataFrame({\n",
        "  'class_names': class_names,\n",
        "  'values': y_pred_probs[idx]\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZCqhv7um_I5",
        "outputId": "582b972c-0966-44f7-ab54-a46d33ae2fc6"
      },
      "source": [
        "#250 269\n",
        "isi_berita = '''hp keluaran apple adalah iphone. olahraga sangat menyenangkan dan menyehatkan'''\n",
        "predstext = get_predictions_text(model, isi_berita , MAX_LEN, 0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHJ71aK3nj_1",
        "outputId": "a67c2942-d56c-4ff5-9227-57e4b840c788"
      },
      "source": [
        "print(predstext)\n",
        "\n",
        "#for a in range(len(predstext)):\n",
        "#  index = 0\n",
        "#  print(class_names[index])\n",
        "#  index+=1\n",
        "  #if (float(class_names[a])>threshold):\n",
        "  #  print(class_names[a])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['tekno', 'lifestyle']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTZjdpvruX7-",
        "outputId": "d00c57ab-c671-4c6f-f075-e177606f2f78"
      },
      "source": [
        "predstext[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.08990626, 0.17553002, 0.99940014, 0.05432682, 0.17469932],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPac-ySt-aA3",
        "outputId": "1579311e-8e8c-4cfd-ab6e-57dacd199d76"
      },
      "source": [
        "print(\"\\n\".join(wrap(review_text)))\n",
        "print()\n",
        "print(f'True sentiment: {class_names[class_ids]}')\n",
        "print(class_names)\n",
        "print(class_ids)\n",
        "print(class_pred)\n",
        "print(predprobs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "timnas u indonesia harus tel pil pahit turnamen piala asia u tahun\n",
            "resmi batal konfederasi sepak bola asia afc afc resmi batal piala asia\n",
            "u konfirmasi lalu surat kirim satu sepak bola seluruh indonesia pssi\n",
            "jumat adapun faktor buat turnamen sebut batal wabah covid landa\n",
            "seluruh dunia piala asia u kemudian ganti nama jadi piala asia u\n",
            "langsung tahun uzbekistan tetap jadi tuan rumah dengan batal kompetisi\n",
            "main timnas u rasa kecewa apalagi skuad garuda muda laku rangkai pusat\n",
            "latih tc mentas turnamen sepak bola antarnegara asia itu namun main\n",
            "minta tak tak patah semangat jalan karier bagas kaffa dkk indonesia\n",
            "panjang lebih ada beberapa turnamen tingkat usia lain bisa ikut main\n",
            "pasuk level timnas senior kami coba terus beri motivasi karier masih\n",
            "panjang kata asisten latih timnas u nova arianto kutip bolasportcom\n",
            "sabtu banyak event akan hadir mereka main timnas u lanjut mereka akan\n",
            "ikut saat sampai level timnas senior jelas pria usia tahun itu belum\n",
            "ajang piala dunia u langsung indonesia telah putus tunda hingga adapun\n",
            "indonesia masih akan tindak tuan rumah piala dunia u abdul rohman\n",
            "\n",
            "True sentiment: sports\n",
            "['edukasi', 'tekno', 'sports', 'health', 'lifestyle']\n",
            "tensor(2)\n",
            "tensor(2)\n",
            "tensor([-2.1493, -1.6253,  7.4580, -2.5486, -2.0651])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp9K8yB84UeS"
      },
      "source": [
        "# **LOAD BERT MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-RKrU9fhxxs"
      },
      "source": [
        "class_names = ['edukasi', 'tekno', 'sports', 'health', 'lifestyle']\n",
        "class Klasifikasi(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(Klasifikasi, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_BAHASA)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      return_dict=False\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPgdUyr8h8RJ"
      },
      "source": [
        "PRE_TRAINED_MODEL_BAHASA =  'indobenchmark/indobert-base-p1'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_BAHASA)\n",
        "\n",
        "MAX_LEN = 400\n",
        "BATCH_SIZE = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nq41RKQtchfE",
        "outputId": "488b752a-149c-4dbd-dfa9-c9f32bfae607"
      },
      "source": [
        "\n",
        "\n",
        "#model = model.to(device)\n",
        "model = Klasifikasi(len(class_names))\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Skripsi/model/isiberita_epoch4.bin\", map_location=torch.device('cpu')))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zypBjt0uighg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f036e67-3d07-4f40-c1e3-adb355e56671"
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Klasifikasi(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (drop): Dropout(p=0.3, inplace=False)\n",
              "  (out): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM75F-1YEczV"
      },
      "source": [
        "isi_berita = '''\n",
        "timnas u indonesia harus tel pil pahit turnamen piala asia u tahun\n",
        "resmi batal konfederasi sepak bola asia afc afc resmi batal piala asia\n",
        "u konfirmasi lalu surat kirim satu sepak bola seluruh indonesia pssi\n",
        "jumat adapun faktor buat turnamen sebut batal wabah covid landa\n",
        "seluruh dunia piala asia u kemudian ganti nama jadi piala asia u\n",
        "langsung tahun uzbekistan tetap jadi tuan rumah dengan batal kompetisi\n",
        "main timnas u rasa kecewa apalagi skuad garuda muda laku rangkai pusat\n",
        "latih tc mentas turnamen sepak bola antarnegara asia itu namun main\n",
        "minta tak tak patah semangat jalan karier bagas kaffa dkk indonesia\n",
        "panjang lebih ada beberapa turnamen tingkat usia lain bisa ikut main\n",
        "pasuk level timnas senior kami coba terus beri motivasi karier masih\n",
        "panjang kata asisten latih timnas u nova arianto kutip bolasportcom\n",
        "sabtu banyak event akan hadir mereka main timnas u lanjut mereka akan\n",
        "ikut saat sampai level timnas senior jelas pria usia tahun itu belum\n",
        "ajang piala dunia u langsung indonesia telah putus tunda hingga adapun\n",
        "indonesia masih akan tindak tuan rumah piala dunia u abdul rohman\n",
        "'''\n",
        "model = model.eval()\n",
        "preds_label = []\n",
        "encoded_review = tokenizer.encode_plus(\n",
        "  isi_berita,\n",
        "  max_length=MAX_LEN,\n",
        "  add_special_tokens=True,\n",
        "  return_token_type_ids=False,\n",
        "  padding='max_length',\n",
        "  return_attention_mask=True,\n",
        "  truncation=True,\n",
        "  return_tensors='pt',\n",
        ")\n",
        "\n",
        "input_ids = encoded_review['input_ids'].to(device)\n",
        "attention_mask = encoded_review['attention_mask'].to(device)\n",
        "\n",
        "output = model(input_ids, attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw5cRCOOj6TK",
        "outputId": "4a647ff0-5210-43ec-c7fd-6d12348a7e12"
      },
      "source": [
        "predstext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tekno', 'lifestyle']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1At0sqhcOkC-"
      },
      "source": [
        "def get_predictions_test(model, data_loader):\n",
        "  model = model.eval()\n",
        "  temp = []\n",
        "  real_values = []\n",
        "  probs_labels = []\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      real_values.extend(targets)\n",
        "      probs_labels.extend(F.sigmoid(outputs))\n",
        "  \n",
        "  probs_labels = torch.stack(probs_labels).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "\n",
        "  return probs_labels, real_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlhe7odPQoq2",
        "outputId": "f7fddff3-eaa2-4bc7-d5e2-9b1f1a0a9190"
      },
      "source": [
        "y_pred_probs, y_test = get_predictions_test(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMisJwqOUwF2"
      },
      "source": [
        "def SubmissionTest(df, y_pred_prob):\n",
        "  temp_labels=[]\n",
        "  for a in range(len(y_pred_prob)):\n",
        "    text = \"\"\n",
        "\n",
        "    indeks = 0\n",
        "    for b in range(len(y_pred_prob[a])):\n",
        "      if(y_pred_prob[a][b]>0.5):\n",
        "        indeks+=1\n",
        "        if indeks==1:\n",
        "          text += \"%s\"%(class_names[b])\n",
        "        else:\n",
        "          text += \", %s\"%(class_names[b])\n",
        "        #print(text)\n",
        "    temp_labels.append(text)\n",
        "\n",
        "  test_df['y_true_prediction'] = temp_labels\n",
        "  \n",
        "  test_df.drop(columns='kategori_int')\n",
        "  test_df.to_csv('submission.csv', index=False, encoding='utf-8')\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEPWvbcSfWHK",
        "outputId": "0bcb9896-e976-49fb-ad1e-38a1860604b2"
      },
      "source": [
        "SubmissionTest(test_df, y_pred_probs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80_nOQT7hefq",
        "outputId": "e4eabbf0-0272-4323-aa1c-43d2a5fce2a9"
      },
      "source": [
        "y_pred_probs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0041, 0.6452, 0.0010, 0.0017, 0.6047])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}